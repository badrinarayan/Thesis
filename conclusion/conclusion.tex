%!TEX root = ../dissertation.tex
\chapter{Conclusion and Future Work} %(fold)
\label{chap:conclusion}

The convex optimization perspective on problems in signal processing and systems
holds a lot of promise. This work forms the rudiments of continuous sparse
recovery which underlies a number of estimation problems in parametric signal
processing, systems theory and machine learning. In fact, using the atomic norm
framework introduced by~\cite{crpw}, we were able to revisit classical problems
and empirically and theoretically demonstrate performance favorable to and
comparable to the state-of-art.

The primary contribution of this thesis is a novel way to analyze regularization
with convex penalties for the case of continuous dictionaries that occur
naturally in several problems. In fact, we saw that the recovery performance is
more tied to the target we wish to estimate and not the arbitrary coherence of
the atomic set. This turns the attention away from global properties of
dictionaries which answer the question ``Which dictionaries enable sparse
recovery using relaxations?'' to properties of signals. By employing a local
coherence condition characterized by minimum separation between frequencies for
line spectral signals, we instead ask question ``Which target signals are
recoverable using convex relaxations?''. 

However, we have only scratched the surface of what is possible by continuing
this line of inquiry. In~\cite{cs_otg}, the authors extended the work descrived
in this thesis to show that using the atomic norm framework, we could achieve
continuous compressed sensing using Fourier measurements. My theoretical
analysis of the optimal performance for continuous sparse recovery is heavily
tied to Fourier measurements and properties of trigonometric polynomials derived
in ~\cite{CandesGranda,cg_noisy}. Future work should explore abstract conditions
when we can guarantee similarly optimal rates.

Though chapter~\ref{chap:linespect} makes significant progress at understanding
the theoretical limits of line-spectral estimation and superresolution, the
bounds could still be improved. For instance, it remains open as to whether the
logarithmic term in Theorem~\ref{main} can be improved to $\log(n/k)$. Deriving
such an upper bound or improving our minimax lower bound would provide an
interesting direction for future work. Additionally, it is not clear if our
localization bounds in Theorem~\ref{support} have the optimal dependence on the
number of sinusoids $k$. For instance, one expects that the condition on signal
amplitudes for approximate support recovery should not depend on $k$, by
comparison with similar guarantees that have been established for
Lasso~\cite{coherence}. It is reasonable to conjecture that for a large enough
regularization parameter, there should will be no spurious recovered frequencies
in the solution. That is, there should be no non-zero coefficients in the ``far
region'' $F$ in Theorem~\ref{support}. Future work should investigate whether
better guarantees on frequency localization are possible.

The analysis in chapter~\ref{chap:sysid} also focused on the particular case of
sampling the frequency response at regular intervals. While this example
contains the critical ingredients to computing convergence rate, it is not
straightforward to extend to the other sampling models described in
Section~\ref{sec:computation}. A useful line of study would extend this analysis
to estimating transfer functions from pairs of input-output time series. The
rates derived demonstrate that the DAST algorithm is asymptotically consistent,
but is quite crude. It may be possible to improve the rates by leveraging more
of the geometry of the set of single-pole transfer functions. It would be
interesting to find reasonable lower-bounds on the reconstruction error from
limited measurements, and to see how close we can match these worst-case
estimates via a new analysis.

Algorithmically, the success of the method depends on whether we can efficiently
compute atomic norms. We saw that we can sometimes get exact semidefinite
characterizations and provided scalable algorithms. We also saw that
discretization approximates the solution although it does not yield as sparse a
solution as the exact approach. An interesting line of work is showing how to
bootstrap an approximate solution from a fast discretization algorithm and
incrementally increase the accuracy of the solution. While gridding enables us
to quickly solve atomic norm problems, a main drawback is that we can never
exactly localize the true composing atoms without an extremely fine grid. One
recent proposal to enable such a localization uses a linearization technique to
simultaneously fit a model on the grid points and at the derivatives of the
transfer functions at these grid points~\cite{Simoncelli11}. It would be
interesting to see whether this argument can give rigorous guarantees of
localization for general atomic sets.

Analysis of the convergence of greedy algorithms provides yet another fruitful
direction for scaling the performance of atomic norm regularization. While there
is already some interesting work on employing greedy methods for atomic
norms~\cite{tewari2011greedy,raogreedy}, the proof of convergence depends upon
global restricted smoothness property. It would be interesting to pursue
improvements to this assumption.

% \section{Publications}
% 
% This dissertation is a result of a collaboration with Prof. Benjamin Recht, and
% postdoctoral students, Dr. Gongguo Tang and Dr. Parikshit Shah at the Wisconsin
% Institute of Discovery. Below, we provide a list of publications that composes
% much of this work.
% 





% chapter conclusion (end)
