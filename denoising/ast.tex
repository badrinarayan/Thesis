%!TEX root = ../dissertation.tex
\chapter{Simple Models and Atomic Norms}
\label{chap:ast}

The foundation of our technique consists of extending work on \emph{atomic
norms} in linear inverse problems in \cite{crpw}. In this work, the authors
describe how to reconstruct models that can be expressed as sparse linear
combinations of \emph{atoms} from some basic set $\A.$ The set $\A$ can be very
general and is not assumed to be finite. For example, if the signal is known to
be a low rank matrix, $\A$ could be the set of all unit norm rank-$1$ matrices,
since a low rank matrix can indeed be written as a sparse combination of such
rank-$1$ atoms.

We will first review the notion of simple models defined in~\cite{crpw} and
describe how this generalizes various notions of sparsity and structure. We will
also show how to use an atomic norm penalty to denoise a signal known to be a
sparse nonnegative combination of atoms from a set $\A$. Atomic norms provide a
natural convex penalty function for discouraging specialized notions of
complexity. These norms generalize the $\ell_1$ norm for sparse vector
estimation~\cite{candes06} and the nuclear norm for low-rank matrix
reconstruction~\cite{Recht10,CandesRecht09}.

Our first contribution is an abstract theory of denoising with atomic norms. We
show a unified approach to denoising with the atomic norm that provides a
standard approach to computing low mean-squared-error (MSE) estimates. We show
how certain Gaussian statistics and geometrical quantities of particular atomic
norms are sufficient to bound estimation rates with these penalty functions. Our
approach is essentially a generalization of the Lasso~\cite{tibshirani96,chen98}
to infinite dictionaries.

\subsection*{Organization of this chapter}

The denoising problem is obtaining an estimate $\hat{x}$ of the signal $x^\star$
from $y = x^\star + w$, where $w$ is additive noise. We make the structural
assumption that $x^\star$ is a sparse non-negative combination of points from an
arbitrary, possibly infinite set $\A \subset \C^n$. This assumption is very
expressive and generalizes many notions of sparsity~\cite{crpw}. The atomic norm
$\vnorm{\cdot}_\A$, introduced in \cite{crpw}, is a penalty function specially
catered to the structure of $\A$ as we shall examine in depth in
Section~\ref{sec:ast:preliminaries}, and is defined as:
\begin{equation}
\label{def-atnorm}
\vnorm{x}_\A = \inf\left\{t > 0 ~\middle|~x \in t \conv(\A) \right\}.
\end{equation}
 where $\conv(\A)$ is the convex hull
of points in $\A.$ We analyze the denoising performance of an estimate that uses
the atomic norm to encourage sparsity in $\A$.

\paragraph*{Decomposition.} In Section~\ref{sec:decomposition}, we start with
vectors $x^\star$ that may be written as a sparse nonnegative combination of
elements from the atomic set $\A$ and ask how one might certify that $x^\star$
has a unique sparsest decomposition in terms of the atoms. We will see how the
existence of certain vectors in the dual space can reveal the composing atoms
and certify uniqueness of the sparsest decomposition under some mild technical
conditions.

\paragraph*{Denoising.} In Section \ref{sec:abstract-denoising},
we will characterize the performance of the estimate $\hat{x}$ that solves

\begin{equation}
\label{AST}\mathop{\textrm{minimize}}_x \frac{1}{2} \vnorm{x - y}_2^2 + \tau \vnorm{x}_\A.
\end{equation}
where $\tau$ is an appropriately chosen regularization parameter, and $y =
x^\star + w$ is a a vector of noisy measurements. We provide an upper bound on
the MSE of the estimate when the noise statistics are known. Before we state the
theorem, we note that the dual norm $\vnorm{\cdot}_\A^*$, corresponding to the
atomic norm, is given by
\[
 \vnorm{z}_{\A}^* = \sup_{a \in \A}{\vabs{z}{a}},
\]
where $\vabs{x}{z} = \Re(z^*x)$ denotes the real inner product.  

\begin{theorem}[Universal Denoising Guarantee]
\label{cor:expected-mse}

Suppose we observe the signal $y = x^\star + w$ where $x^\star \in \C^n$ is a
sparse nonnegative combination of points in $\A$. The estimate $\hat{x}$ of
$x^\star$ given by the solution of the atomic soft thresholding problem
\eqref{AST} with $\tau \geq \E \vnorm{w}_\A^*$ has the expected (per-element)
MSE
\[ 
\frac{1}{n}\E \vnorm{\hat{x} - x^\star}_2^2 \leq \frac{\tau}{n}\vnorm{x^\star}_\A
\]

\end{theorem}

This theorem implies that when $\E \vnorm{w}_\A^*$ is $o(n)$, the estimate
$\hat{x}$ is consistent.

\paragraph*{Choosing the regularization parameter.} In 
Section~\ref{sec:dual-atomic-bounds}, we discuss the choice of regularization
parameter $\tau$. Our lower bound on $\tau$ is in terms of the expected dual norm of the noise process $w$, equal to
 \[
 	\E \vnorm{w}_\A^* = \mathbb{E}[\sup_{a\in \A} {\vabs{w}{a}}].
 \]
That is, the optimal $\tau$ and achievable MSE can be estimated
by studying the extremal values of the stochastic process indexed by the atomic
set $\A$.

\paragraph*{Accelerated Convergence Rates} The MSE rates given by
Theorem~\ref{cor:expected-mse} are not the best possible. In
Section~\ref{sec:convergence-rate}, we describe a condition on the atomic sets
that enables accelerate convergence rates. This is a generalization of the weak
compatibility criterion~\cite{degeer} and unifies the condition for fast rates
for several atomic sets. In particular, under this assumption, we show that we
can recover fast convergence rates published in literature for sparse vectors
and low rank matrices.

% \paragraph*{Local Rates}
% \todo{TODO: Modulus of Convexity, Bregman divergences and fast rates.}

\section{Preliminaries}
\label{sec:ast:preliminaries}

The notion of atomic sets and simple models introduced in~\cite{crpw} subsumes
several structures in high dimensional geometry, including sparse
vectors, low rank matrices, discrete measures, linear systems of small order.
The authors also define a notion of an \emph{atomic norm} as a natural convex
penalties for encouraging simplicity with respect to these structures. In this
section, we review these preliminaries.

Let $F$ be $\C$ or $\R$. Consider a possibly infinite set $\A \subset F^n$ which
we shall dub the \emph{atomic set}. Every signal we shall consider is composed
of a combination of \emph{atoms} from this dictionary. We say a target signal
$x^\star \in F^n$ is \emph{simple}, if it can be written as a nonnegative
combination of small subset $T \subset \A$ of atoms. We call $T$, a
\emph{support} of $x^\star$. If the number of elements $k = |T|$ in the support
is small relative to the dimension $n$ of the signal, we say $x^\star$ is a
($k$) \emph{simple} combination of atoms.

Allowing the dictionary of atomic set to be infinite offers tremendous
flexibility in the kind of high dimensional structures that can be modeled. To
see this, we shall see specific instances of the setup, that will serve as a
motivation for our framework. In subsequent chapters, we will revisit some of
these examples.

\subsection{Examples}

\paragraph{Estimating Nearly Black Objects}

Consider the estimation of the \emph{non-negative} vector $x^\star \in \R^m$
from the noisy observations $y = x^\star + w$ with the assumption that $x^\star$
is nearly black --- i.e., most of its entries are zero. This problem was
considered in \cite{DonohoNearlyBlack92,JohnstoneSparse94}, in order to shed
light on the superresolution properties of the maximum entropy estimator for
nearly black MRI images. In our framework, such an $x^\star$ is a sparse
combination of $\A = \{ \pm e_1, \dots, \pm e_n\}$ where $e_i$ is the $i$th
canonical unit vector.

\paragraph{Compressed Sensing and Sparse Recovery}

Many phenomena observed in high dimension are sparse in a suitable dictionary.
For instance, natural images expanded in a wavelet basis are well approximated
by a sparse combination of wavelet coefficients. This means the data is highly
redundant but it is actually sparse when suitably transformed. Compression
techniques exploit this redundancy and represent the same high dimensional
vector with fewer bits with no loss of accuracy. However, it is wasteful to
acquire several linear measurements of the data only to compress them and throw
away many bits. Compressed Sensing exploits the ability of the sparse recovery
algorithms to directly estimate the $p$ dimensional sparse vector from $n$
limited random linear measurements where $n \ll p$.

Let $\theta^\star \in \R^p$ be sparse in a known orthogonal basis $\Phi \in
\R^{p \times p}$. Suppose we make $n$ observations $x^\star_i =
\vabs{\psi_i}{\theta^\star}$ where $\{\psi_i\}_{i=1}^n$ are randomly chosen
Gaussian vectors. Then, $x^\star$ is a simple combination of atoms from the
atomic set given by $\setof{\pm \Psi^T \Phi_j}{j=1,\ldots,p}$ where $\Psi \in
\R^{p \times n}$ is the matrix with $\psi_i$ for its columns.

\paragraph{Low Rank Matrix Estimation and Matrix Completion}

The problem of finding a minimum rank matrix from incomplete data arises in a
number of applications including collaborative filtering, system identification
and document classification. The data available to us could be a fraction of the
entries of the matrix, or in general any set of linear measurements of the
matrix. Let $\A$ be the one dimensional manifold composed of all unit norm
rank-$1$ matrices. Then a low rank matrix $x^\star$ can be expressed as a sparse
combination of rank-$1$ matrices, and consequently the observations of this
matrix can be expressed as a sparse combination of observations due to these
rank-$1$ matrices. In this example, note that it is not sufficient to consider a
finite atomic set, since the set of all low rank matrices are not sparse in any
finite dictionary.

\paragraph{Line Spectral Estimation} Line Spectral Estimation concerns with the
recovery of signals whose spectrum consists only of a finite number of
frequencies. In other words, line spectral signals are simply a finite mixture
of complex exponentials. An important signal processing task is the ability to
recover the frequencies and amplitudes of the complex exponentials from a
limited number of time samples. For each frequency $f$ in $[-W,W]$, let $a(f)$
denote the vector of observed time samples. Then the set $\A$ of different
observations $a(f) \in \C^m$ due to each frequency $f$ comprises the atomic set
for samples of line spectral signals.

\paragraph{System Identification} An important task in modeling is the
identification of a linear system with the minimum number of states from its
input and output. Let $\A$ denote the set of transfer function corresponding to
single pole systems. Then an LTI system with small order has a transfer function
which is a sparse combination of atoms from $\A$. Therefore the task of system
identification reduces to finding the comprising atoms given linear
measurements.

The last two examples can be seen as an instance of continuous sparse recovery
problem, since there are continuously many atoms. In most previous literature,
sparse recovery problems are analyzed only in discrete settings. In fact, in
previous literature, authors have advocated a discretization approach which
amounts to working with a finite set $\tilde{\A}$ instead of the infinite atomic
set $\A$. However, a finer gridding of the atomic set results in a more coherent
dictionary and the usual compressed sensing theoretical guarantees degrade with
the grid size. We bypass this by directly analyzing the continuous case using a
different approach. We also show that the coherence of the dictionary is not a
fundamental limitation and that we can get increasingly accurate results by
finer and finer discretizations. 

In succeeding chapters, we will concentrate on the application of the atomic
norm framework to these examples. It is not our goal to provide an exhaustive
catalogue of examples of the framework and we refer the interested reader
to~\cite{crpw}, which contains several more interesting applications.

\subsection{Atomic Norm}

\begin{definition}[Atomic Norm]
The atomic norm $\vnorm{\cdot}_\A$ of $\A \subset F^n$ is the Minkowski
functional (also called the gauge function) associated with $\conv(\A)$ (the
convex hull of $\A$):
\begin{equation}
	\label{defatnorm} \vnorm{x}_\A = \inf\left\{t > 0 ~\middle|~ x \in t \conv(\A) \right\}. 
\end{equation}
\end{definition}

The gauge function is a norm in $F^n$ if $\conv(\A)$ is compact, centrally
symmetric, and contains a ball of radius $\epsilon$ around the origin for some
$\epsilon>0$. Nevertheless, we shall call it atomic norm even if it is not as a
norm, as the authors in~\cite{crpw} do. When $\A$ is the set of unit norm
$1$-sparse elements in $\C^n$, the atomic norm $\vnorm{\cdot}_\A$ is the
$\ell_1$ norm~\cite{candes06}. Similarly, when $\A$ is the set of unit norm
rank-$1$ matrices, the atomic norm is the nuclear norm~\cite{Recht10}.
In~\cite{crpw}, the authors showed that minimizing the atomic norm subject to
equality constraints provided exact solutions of a variety of linear inverse
problems with nearly optimal bounds on the number of measurements required.

While not necessary for the definition of the atomic norm, we will assume some
weak regularity conditions in the rest of the thesis. These are satisfied for a
number of examples and they allow us to phrase our theorems without restating
assumptions explicitly each time. We will assume that the atomic set $\A$ satisfies the following properties:
\begin{enumerate}
	\item No atom can be written as a conical combination of other atoms in 		$\A$. In other words, we assume that $ a \not\in \conv{\A \backslash \set{a}}$ for every $a \in \A$. This guarantees that elements in $\A$ are the extreme points of the set $\conv(\A)$.
	\item The set $\A$ is a closed subset of $F^n$. This assumption is always true for finite sets.
	\item The spark of a set of vectors $\A \subset F_n$ is defined as the smallest number $\sigma$ such that there exists a subcollection of $\sigma$ elements of $\A$ which are linearly dependent. We will assume that the spark of $\A$ is $n$. As a consequence, any decomposition of a vector $x^\star$ into $<n/2$ atoms is necessarily unique.~\cite{spark}
	\item We will assume that $\conv(\A)$ has a non-empty interior to avoid degenerate cases.
\end{enumerate}


\subsection{Dual Atomic Norm}

Corresponding to the atomic norm, we can define the dual atomic norm, which is
given by
\begin{equation}
  \label{defdualatnorm}
  \vnorm{z}_{\A}^* = \sup_{\vnorm{x}_\A \leq 1}{\vabs{x}{z}},
\end{equation}
which implies
\begin{equation}
  \label{holder}
  \vabs{x}{z} \leq \vnorm{x}_\A \vnorm{z}_\A^*.
\end{equation}

The supremum in \eqref{defdualatnorm} is achievable, namely, for any $x$, there
is a $z$ that achieves equality. Since $\A$ contains all extremal points of
$\{x: \|x\|_\A \leq 1\}$, we are guaranteed that the optimal solution will
actually lie in the set $\A$ (See~\cite{bonsall} for a proof). So, we can write

\begin{equation}
\vnorm{z}_{\A}^* = \sup_{a \in \A}{\vabs{a}{z}}.\label{bonsall}
\end{equation}
The dual norm will play a critical role throughout, as our asymptotic error
rates will be in terms of the dual atomic norm of noise processes. The dual
atomic norm also appears in the dual problem of \eqref{AST}.

\section{Decomposition}
\label{sec:decomposition}

A natural question is whether it is always possible to recover the composing
atoms of a simple $x^\star$. As stated, the problem is ill-posed, for there
could many decompositions. However, under our assumption, there is a unique
sparsest decomposition of $x^\star$ provided there is a decomposition that is at
most $n/2$ sparse. So, the question is well posed as long as we start with such
an $x^\star.$

The goal of atomic norm decomposition is to write $x^\star$ in terms of
composing atoms such that the sum of the coefficients is the atomic norm. We
call such a decomposition as an atomic norm achieving decomposition. While not
always true, the decomposition that achieves the atomic norm is often the
sparsest and in this section provide some geometric insight to when this is
true. We also describe a procedure to find such a decomposition using the dual.

\subsection{Dual Certificate}

A useful device is that of a dual certificate, which is one of the subgradients
of the atomic norm. For a convex differentiable function $f$, the gradient
defines a global linear under approximator for the function. Subgradients
generalize this property to nonsmooth convex functions by characterizing all
linear under approximators of the function. A vector $q$ is said to be in the subgradient of $f$ at $x$ if for every $y,$
\[
	f(y) \geq f(x) + \vabs{q}{y-x}.
\]
When $f$ is differentiable at $x$, the set of subgradients at $x$ is simply the
singleton set containing the gradient at $x$. It is easy to verify that $q$ is a
subgradient of $\vnorm{\cdot}_\A $ at $x^\star$ if and only if $\langle q,
x^\star \rangle = \vnorm{x^\star}_\A$ and $\vnorm{q}_\A^* \leq 1.$ A
decomposition of a vector $x^\star$ gives an upper bound for its atomic norm.
Subgradients in the dual space can provide a dual certificate of the optimality
of a decomposition if it produces a matching lower bound.

\begin{definition}[Dual Certificate]\label{def:dual-certificate}
A vector $q$ is a dual certificate for the support $T \subset \A$ if 
$\vabs{q}{a} = 1$ for every $a \in T$ and $\vabs{q}{a} \leq 1$ whenever $a \not\in T$. Furthermore, the vector $q$ is called a strict dual certificate if $\vabs{q}{a} < 1$ for every $a \not\in T.$
\end{definition}

This has a pleasing geometric interpretation. By definition of the dual
certificate, for every non-empty set $T$, $\vnorm{q}_\A^* = \sup_{a \in \A}
\vabs{q}{a} = 1$, which guarantees that all the atoms (and hence $\conv(\A)$)
lie on one side of a half plane determined by $q$, and also that atoms in $T$
(and hence $\conv(T)$) intersect this hyperplane. Geometrically, this says that
$q$ is a supporting hyperplane for the exposed face $\conv(T)$ of the convex set
$\conv(\A).$ The following proposition indicates why this is important.

\begin{prop}
\label{prop:dual-certificate-is-subgrad}
Suppose $x^\star$ can be written in terms of atoms in $T$, i.e., $x^\star =
\sum_{a \in T} c_a a$ for some $\set{c_a} > 0$. If $q$ is a dual certificate for
$T,$ then, $\sum_{a \in T} c_a a$ is an atomic norm achieving decomposition of
$x^\star$ and $q$ is a subgradient of the atomic norm at $x^\star.$
\end{prop}
\begin{proof}
	Then, by the definition of atomic norm $\vnorm{x^\star}_\A \leq \sum_{a \in T} c_a$. Now, 
	\[
		\vabs{q}{x^\star} = \sum_{a \in T} c_a\vabs{q}{a} = \sum_{a \in T} c_a
	\]
	But 
	\[
		\vabs{q}{x^\star} \leq \vnorm{q}_\A^* \vnorm{x^\star}_\A = \vnorm{x^\star}_\A.
	\]
	Combining these two equations, we get $\sum_{a \in T} c_a = \vnorm{x^\star}_\A =
	\vabs{q}{x}$, which completes the proof.
\end{proof}

Conversely, the presence of a \emph{strict} dual certificate which is also a
subgradient of $\vnorm{\cdot}_\A$ at $x^\star$ guarantees that $x^\star$ may be
written as a combination of atoms in $T.$ 

\begin{prop}\label{prop:certificate-support}
Suppose $q$ is a subgradient of $\vnorm{\cdot}_\A$ at $x^\star \in \cone(\A)$
and is a strict dual certificate for $T \subset \A$, then $x^\star$ has an
atomic norm achieving decomposition in terms of atoms from $T,$ i.e., there
exists $c_a > 0$ such that $x^\star = \sum_{a \in T}c_a a$ with
$\vnorm{x^\star}_\A = \sum_{a \in T}c_a$.
\end{prop}

Under our assumptions (that $\A$ has full spark), we are guaranteed that this is
the unique sparsest decomposition of $x^\star$ in terms of the atoms provided
$|T| < n/2$. This is especially useful as it allows us to determine a support of
$x^\star$ purely in terms of the properties of the dual. For a generic set,
there is no guarantee that the atomic norm achieving decomposition is $n/2$
sparse and therefore that it recovers the \emph{correct} support. However, we
have a rich theory now that this is indeed true for many interesting
cases\cite{donoho2006most,candes06,candes2005decoding}.

Returning to our geometric interpretation, the existence of a dual certificate
guarantees that $x^\star/\vnorm{x}_\A$ is in an exposed face of $\conv(\A)$. Due
to our assumptions on $\A$, the faces are simplicial and there is a unique way
of writing $x^\star$ as a combination of vertices in the face. When the face is
also low dimensional, the decomposition of $x^\star$ is unique. So, we can
always recover the support of a sparse vector provided all sparse combinations
lie on low dimensional exposed simplicial faces. Again, while this may not
always be true, it is indeed true with large probability for random
constructions of the atomic set and we refer the interested reader
to~\cite{neighborliness} for this geometric interpretation of sparse recovery.

In practice, it may be hard to find a \emph{strict} dual certificate by
optimization. However, if $\A$ has full spark like we assumed, any dual
certificate will suffice to find a set of atoms for a decomposition that
achieves the atomic norm. Given a simple $x^\star$, our recipe involves solving
the semi-infinite program
\begin{equation}
\label{eq:ast:dual:support}
\begin{split}
	\maximize_q~ & \vabs{q}{x^\star}\\
	\text{subject to } & \vabs{q}{a} \leq 1, \text{ for every } a \in \A.
\end{split}
\end{equation}
to obtain a solution $\hat{q}$ which is one of the dual certificates of the
support and a subgradient of $\vnorm{\cdot}_\A$ at $x^\star$. However,
$\vabs{q}{a}$ is $1$ only for at most $n$ atoms under our assumptions. In fact,
if $\vabs{q}{a}$ has the same value for $n+1$ atoms $a_1, \dots, a_{n+1}$, we
have $\vabs{q}{a_{i+1}-a_i} = 0$ for $i=1,\ldots,n$ and using the full spark
assumption, we can conclude that $q$ is identically zero. This means $q$ is in
fact a strict dual certificate for a support comprising at most $n$ atoms which
certifies that the atomic norm achieving decomposition can be composed in terms
of the atoms where $\vabs{q}{a}=1.$ The coefficients of the decomposition can be
determined by solving a linear system.

\section{Denoising}
\label{sec:abstract-denoising}

Now, let us look at a scheme that is robust to measurement noise. To set up the
atomic norm denoising problem, suppose we observe a signal $y = x^\star + w$
where $w$ is a noise vector and that we know \emph{a priori} that $x^\star$ can
be written as a linear combination of a few atoms from $\A$. One way to estimate
$x^\star$ from these observations would be to search over all short linear
combinations from $\A$ to select the one which minimizes $\vnorm{y- x}_2$.
However, this could be formidable: even if the set of atoms is a finite
collection of vectors, this problem is the NP-hard SPARSEST VECTOR
problem~\cite{Natarajan95}.

On the other hand, the problem~\eqref{AST} is convex, and reduces to many
familiar denoising strategies for particular $\A$. The mapping from $y$ to the
optimal solution of \eqref{AST} is called the proximal operator of the atomic
norm applied to $y$, and can be thought of as a soft thresholded version of $y$.
Indeed, when $\A$ is the set of $1$-sparse atoms, the atomic norm is the
$\ell_1$-norm, and the proximal operator corresponds to \emph{soft-thresholding}
$y$ by element-wise shrinking towards zero~\cite{donoho1995noising}. Similarly,
when $\A$ is the set of rank-$1$ matrices, the atomic norm is the nuclear norm
and the proximal operator shrinks the singular values of the input matrix
towards zero.

We now establish some universal properties about the problem~\eqref{AST}.
First, we collect a simple consequence of the optimality conditions in a lemma:

\begin{lemma}[Optimality Conditions]
\label{lem:optimality-conditions}
$\hat{x}$ is the solution of \eqref{AST} if and only if\\
\emph{(i)}  $\vnorm{y - \hat{x}}_\A^* \leq \tau$,
\emph{(ii)} $\vabs{y - \hat{x}}{\hat{x}} = \tau \vnorm{\hat{x}}_\A.$
\end{lemma}
\begin{proof}
The function $f(x) = \tfrac{1}{2}\vnorm{y - x}_2^2 + \tau\vnorm{x}_\A$ is minimized at $\hat{x}$, if for all $\alpha \in (0,1)$ and all $x$,
\begin{equation*}
f(\hat{x} + \alpha(x - \hat{x})) \geq f(\hat{x})
\end{equation*}
or equivalently,
\begin{equation}
 \alpha^{-1} \tau \left(\vnorm{\hat{x} + \alpha(x - \hat{x})}_\A - \vnorm{\hat{x}}_\A  \right) \geq \vabs{y - \hat{x}}{x - \hat{x}} - \frac{1}{2}\alpha \vnorm{x - \hat{x}}_2^2\label{eq:limit}
\end{equation}
Since $\vnorm{\cdot}_\A$ is convex, we have
\[
\vnorm{x}_\A - \vnorm{\hat{x}}_\A \geq \alpha^{-1}  \left(\vnorm{\hat{x} + \alpha(x - \hat{x})}_\A - \vnorm{\hat{x}}_\A \right),
\]
for all $x$ and for all $\alpha \in (0,1)$. Thus, by letting $\alpha \to 0$ in \eq{limit}, we note that $\hat{x}$ minimizes $f(x)$ only if, for all $x$,
\begin{equation}
\tau \left( \vnorm{x}_\A - \vnorm{\hat{x}}_\A \right) \geq \vabs{y - \hat{x}}{x - \hat{x}}.\label{eq:subgradient}
\end{equation}
However if \eq{subgradient} holds, then, for all $x$
\begin{align*}
\frac{1}{2}\| y - x \|_2^2 + \tau \vnorm{x}_\A
\geq \frac{1}{2}\| y - \hat{x} + (\hat{x} - x) \|_2^2 + \vabs{y - \hat{x}}{x - \hat{x}} + \tau \vnorm{\hat{x}}_\A
 \end{align*}
implying $f(x) \geq f(\hat{x}).$
Thus, \eq{subgradient} is necessary and sufficient for $\hat{x}$ to minimize $f(x)$. 
\begin{note}
The condition \eq{subgradient} simply says that $\tau^{-1} \left(y - \hat{x} \right)$ is in the subgradient of $\vnorm{\cdot}_\A$ at $\hat{x}$ or equivalently that $0 \in \partial f(\hat{x})$.
\end{note}

We can rewrite \eq{subgradient} as
\begin{equation}
\label{eq:reformulate-subgradient}
\tau \vnorm{\hat{x}}_\A - \vabs{y - \hat{x}}{\hat{x}} \leq \inf_x \left\{ \tau \vnorm{x}_\A - \vabs{y - \hat{x}}{x} \right\}
\end{equation}
But by definition of the dual atomic norm, 
\begin{align}
\label{eq:conjugate-norm}
\sup_x \left\{ \vabs{z}{x} - \vnorm{x}_\A \right\} &=  I_{\left\{ w : \vnorm{w}_\A^* \leq 1\right\}}(z)
 = 
\begin{cases}
0 & \vnorm{z}_\A^* \leq 1\\
\infty & \text{otherwise.}
\end{cases}
\end{align}
where $I_A(\cdot)$ is the convex indicator function. Using this in \eq{reformulate-subgradient}, we find that $\hat{x}$ is a minimizer if and only if $\vnorm{y - \hat{x}}_\A^* \leq \tau$ and $\vabs{y - \hat{x}}{\hat{x}} \geq \tau \vnorm{\hat{x}}_\A$. This proves the theorem.
\end{proof}


\begin{lemma}[Dual Problem] \label{lem:dual-problem} The dual problem
of~\eqref{AST} is
\begin{equation*}
  \label{eq:dual-ast}
  \begin{split} &\maximize_z \frac{1}{2}\left(\vnorm{y}_2^2 - \vnorm{y - z}_2^2\right)\\ &\text{subject to } \vnorm{z}_\A^* \leq \tau. \end{split} \end{equation*} 
The dual problem admits a unique solution $\hat{z}$ due to strong concavity of
the objective function. The primal solution $\hat{x}$ and the dual solution
$\hat{z}$ are specified by the optimality conditions and there is no duality
gap:\\
\emph{(i)} $y = \hat{x} + \hat{z},$ 
\emph{(ii)} $\vnorm{\hat{z}}_\A^* \leq \tau,$
\emph{(iii)} $\vabs{\hat{z}}{\hat{x}} = \tau
\vnorm{\hat{x}}_\A$.  \
\end{lemma}
\begin{proof}
We can rewrite the primal problem \eqref{AST} as a constrained optimization problem:
\[
\begin{split}
&\minimize_{x,u} \frac{1}{2}\vnorm{y - x}_2^2 + \vnorm{u}_\A\\
&\text{subject to }  u = x.
\end{split}
\]
Now, we can introduce the Lagrangian function
\begin{equation*}
L(x,u,z) = \frac{1}{2}\vnorm{y - x}_2^2 + \vnorm{u}_\A + \vabs{z}{x - u}.
\end{equation*} 
so that the dual function is given by
\begin{align*}
g(z) = \inf_{x,u} L(x,u,z)
& = \inf_{x}\left( \frac{1}{2}\vnorm{y - x}_2^2 + \vabs{z}{x} \right) 
+ \inf_{u}\left( \tau\vnorm{u}_\A - \vabs{z}{u} \right)\\
& = \frac{1}{2}\left(\vnorm{y}_2^2 - \vnorm{y - z}_2^2\right) - I_{\left\{ w : \vnorm{w}_\A^* \leq \tau\right\}}(z).
\end{align*}
where the first infimum follows by completing the squares and the second infimum
follows from \eq{conjugate-norm}. Thus the dual problem of maximizing $g(z)$ can
be written as in \eq{dual-ast}.

The solution to the dual problem is the unique projection $\hat{z}$ of $y$ on to
the closed convex set $C = \{ z : \vnorm{z}_\A^* \leq \tau \}$. By projection
theorem for closed convex sets, $\hat{z}$ is a projection of $y$ onto $C$ if and
only if $\hat{z} \in C$ and $\vabs{z - \hat{z}}{y - \hat{z}} \leq 0$ for all $z
\in C$, or equivalently if $\vabs{\hat{z}}{y - \hat{z}} \geq \sup_z\,\vabs{z}{y
- \hat{z}} = \tau \vnorm{y - \hat{z}}_\A.$ These conditions are satisfied for
$\hat{z} = y - \hat{x}$ where $\hat{x}$ minimizes $f(x)$ by Lemma
\ref{lem:optimality-conditions}. Now the proof follows by the substitution
$\hat{z} = y - \hat{x}$ in the previous lemma. The absence of duality gap can be
obtained by noting that the primal objective function at $\hat{x},$
\[
f(\hat{x}) = \frac{1}{2}\vnorm{y - \hat{x}}_2^2 + \vabs{\hat{z}}{\hat{x}}
= \frac{1}{2}\vnorm{\hat{z}}_2^2 + \vabs{\hat{z}}{\hat{x}} = g(\hat{z}).
\]
\end{proof}

A straightforward corollary of this Lemma is a certificate of the support of the
solution to \eqref{AST}.

\begin{corollary}[Dual Certificate of Support]
\label{cor:dual-cert-support}

Suppose for some $S \subset \A$,  $\hat{z}$ is a solution to the dual problem \eq{dual-ast} satisfying
\begin{enumerate}
\item $\vabs{\hat{z}}{a} = \tau$ whenever $a \in S,$
\item $|\vabs{\hat{z}}{a}| < \tau$ if $a \not\in S.$
\end{enumerate}
Then, any solution $\hat{x}$ of \eqref{AST} admits a decomposition $\hat{x} =
\sum_{a \in S}{c_a a}$ with $\vnorm{\hat{x}}_\A = \sum_{a \in S}{c_a}.$

\end{corollary}

Thus the dual solution $\hat{z}$ provides a way to determine a decomposition of
$\hat{x}$ into a set of elementary atoms that achieves the atomic norm of
$\hat{x}$. In fact, one could evaluate the inner product $\left<\hat{z},
a\right>$ and identify the atoms where the absolute value of the inner product
is $\tau$. When the signal-to-noise-ratio (SNR) is high, we expect that the
decomposition identified in this manner should be close to the original
decomposition of $x^\star$ under certain assumptions.


We are now ready to state a proposition which gives an upper bound on the MSE
with the optimal choice of the regularization parameter.

\begin{prop}
\label{prop:main-result}

If the regularization parameter $\tau > \vnorm{w}_\A^*$, the optimal solution
$\hat{x}$ of \eqref{AST} has the MSE
\begin{equation}\label{eq:slow-mse}
\frac{1}{n}\vnorm{\hat{x} - x^\star}_2^2 \leq \frac{1}{n}\left(\tau \vnorm{x^\star}_\A - \vabs{x^\star}{w}\right) \leq \frac{2\tau}{n}\vnorm{x^\star}_\A.
\end{equation} 
\begin{proof}
\begin{align}
	&\vnorm{\hat{x} - x^\star}_2^2 = \vabs{\hat{x}-x^\star}{w-(y -\hat{x})}         \label{expand}\\
	& = \vabs{x^\star}{y-\hat{x}} - \vabs{x^\star}{w} +\vabs{\hat{x}}{w} -\vabs{\hat{x}}{y-\hat{x}}\nonumber\\
	& \leq \tau\vnorm{x^\star}_\A - \vabs{x^\star}{w}+ (\vnorm{w}_\A^* -  \tau)\vnorm{\hat{x}}_\A\label{p1}\\
	& \leq (\tau + \vnorm{w}_\A^*)\vnorm{x^\star}_\A + (\vnorm{w}_\A^* - \tau)\vnorm{\hat{x}}_\A\label{p2}
\end{align}
where for  \eqref{p1} we have used Lemma \ref{lem:optimality-conditions} and \eqref{holder}. 
The theorem now follows from \eqref{p1} and \eqref{p2} since $\tau
> \vnorm{w}_\A^*.$ The value of the regularization parameter $\tau$ to ensure
the MSE is upper bounded thus, is $\vnorm{w}_\A^*.$
\end{proof}
\end{prop}

\noindent\emph{Example: Sparse Model Selection} We can specialize our stability
guarantee to Lasso~\cite{tibshirani96} and recover known results. Let $\Phi \in
\R^{n \times p}$ be a matrix with unit norm columns, and suppose we observe $y =
x^\star + w$, where $w$ is additive noise, and $x^\star = \Phi c^\star$ is an
unknown $k$ sparse combination of columns of $\Phi$. In this case, the atomic
set is the collection of columns of $\Phi$ and $-\Phi$, and the atomic norm is
$\vnorm{x}_{\A} = \min \left\{\vnorm{c}_1: x = \Phi c\right\}$. Therefore, the
proposed optimization problem \eqref{AST} coincides with the Lasso
estimator~\cite{tibshirani96}. This method is also known as Basis Pursuit
Denoising~\cite{chen98}. If we assume that $w$ is a gaussian vector with
variance $\sigma^2$ for its entries, the expected dual atomic norm of the noise
term, $\vnorm{w}_\A^* = \vnorm{\Phi^*w}_\infty$ is simply the expected maximum
of $p$ gaussian random variables. Using the well known result on the maximum of
gaussian random variables~\cite{lr76}, we have $\E\vnorm{w}_\A^* \leq \sigma
\sqrt{2 \log(p)}$. If $\hat{x}$ is the denoised signal, we have from Theorem
\ref{cor:expected-mse} that if $\tau = \E\vnorm{w}_\A^* = \sigma \sqrt{2
\log(p)}$, \[ \frac{1}{n}\E\vnorm{\hat{x} - x^\star}_2^2 \leq \sigma
\frac{\sqrt{2\log(p)} }{n} \vnorm{c^\star}_1, \] which is the stability result
for Lasso reported in \cite{greenshtein04} assuming no conditions on $\Phi$.

\section{Dual Atomic Norm Bounds} % (fold)
\label{sec:dual-atomic-bounds}
As noted in Theorem~\ref{cor:expected-mse}, the optimal choice of the
regularization parameter is dictated by the dual atomic norm of the noise
process. To see why this is the case, let us consider the dual problem to Atomic
Soft Thresholding, given by Lemma~\ref{lem:dual-problem}:
\begin{equation*}
  \begin{split} \minimize_z~&\vnorm{y - z}_2\\
	 \text{subject to }&\vnorm{z}_\A^* \leq \tau.
  \end{split}
\end{equation*} 
Using the optimality conditions in Lemma~\ref{lem:dual-problem}, we see that the
primal solution $\hat{x}$ is a good estimate of the target $x^\star$ if the dual
solution is a good estimate of the noise vector $w$. Thus $\tau$ should be
proportional to noise and may be interpreted as a parameter controlling the
amount of shrinkage towards origin. In fact, when $\tau > \vnorm{y}_\A^*$, we
have $\hat{z} = y$ and $\hat{x} = 0.$ This suggests a choice of $\tau = \E
\vnorm{w}_\A^*$, which corresponds to the strongest mean-squared-error guarantee
in Theorem~\ref{cor:expected-mse}. Specializing this to the case of sparse
vectors in noise, we see that the recommendation $\tau = \E \vnorm{w}_\infty
\approx \sqrt{2 \log(n)}$ coincides with the optimal tuning parameter
in~\cite{donoho1995noising}.

The quantity 
\[
	\E\vnorm{w}_\A^* = \E\sup_{a \in \A} \vabs{w}{a}
\]
where $w \in \mathcal{N}(0,I_n)$ is called the Gaussian width of the atomic set
$\A$. In some cases, the parameterization of the atoms in $\A$ allow the
Gaussian width to be viewed as an extremum of a Gaussian process and thus this
can be estimated using standard tools such as Dudley's
inequality~\cite{ledoux2011probability}, or Talagrand's method of generic
chaining~\cite{talagrand05}.

% section dual-atomic-bounds (end)

\section{Accelerated Convergence Rates} %(fold)
\label{sec:convergence-rate}
In this section, we provide conditions under which a faster convergence rate
can be obtained for AST.
\begin{prop}[Fast Rates]
Suppose the set of atoms $\A$ is centrosymmetric and $\vnorm{w}_\A^*$
concentrates about its expectation so that $P( {\vnorm{w}_\A^*} \geq
\E{\vnorm{w}_\A^*}+t) < \delta(t)$. For $\gamma \in [0, 1]$, define the cone
\begin{align*}
C_\gamma(x^\star,\A) &= \cone(\{z:\vnorm{x^\star+ z}_\A \leq \vnorm{x^\star}_\A + \gamma\vnorm{z}_\A\}).
\end{align*}
Suppose 
\begin{equation}
\label{eq:compatibility}
\phi_\gamma(x^\star,\A) := \inf \left\{ \frac{\vnorm{z}_{2}}{\vnorm{z}_\A}  : z \in C_\gamma(x^\star,\A) \right\} 
\end{equation}
is strictly positive for some $\gamma > {\E\vnorm{w}_\A^*}/{\tau}$. Then
\begin{equation}
\label{eq:fast_rate_phi}
\vnorm{\hat{x}-x^\star}_2^2  \leq \frac{(1+\gamma)^2 \tau^2}{\gamma^2 \phi_\gamma(x^\star,\A)^2}
\end{equation}
with probability at least $1-\delta(\gamma\tau - \E\vnorm{w}_\A^*)$.
\end{prop}

Having the ratio of norms bounded below is a generalization of the Weak
Compatibility criterion used to quantify when fast rates are achievable for the
Lasso~\cite{degeer}. As shown in~\cite{degeer}, this is a weak condition for
fast MSE rates and generalizes the argument for Restricted
Isometry\cite{candes06}, Restricted Eigenvalue~\cite{rest_eig} or Coherence
conditions~\cite{coherence} which are often assumed in literature for deriving
fast rates for the Lasso problem. One difference is that we define the
corresponding cone $C_\gamma$ where $\phi_\gamma$ must be controlled in parallel
with the~\emph{tangent cones} studied in~\cite{crpw}. There, the authors showed
that the mean width of the cone $C_0(x^\star,\A)$ determined the number of
random linear measurements required to recover $x^\star$ using atomic norm
minimization. In our case, $\gamma$ is greater than zero, and represents a
``widening'' of the tangent cone. When $\gamma=1$, the cone is all of $\R^n$ or
$\C^n$ (via the triangle inequality), hence $\tau$ must be larger than the
expectation to enable our proposition to hold.

\begin{proof}
Since $\hat{x}$ is optimal, we have,
\begin{equation*}
\tfrac{1}{2}\vnorm{y - \hat{x}}_2^2 + \tau \vnorm{\hat{x}}_\A \leq \tfrac{1}{2}\vnorm{y - x^\star}_2^2 + \tau \vnorm{x^\star}_\A
\end{equation*}
Rearranging and using~\eqref{holder} gives
\begin{align}
\label{pro:optimality}\tau \vnorm{\hat{x}}_\A &\leq \tau \vnorm{x^\star}_\A + \langle w, \hat{x} - x^\star \rangle\\
\implies \tau\vnorm{\hat{x}}_\A &\leq \tau\vnorm{x^\star}_\A + \vnorm{w}_\A^* \vnorm{\hat{x} - x^\star}_\A.
\end{align} 
Since $\vnorm{w}_\A^*$ concentrates about its expectation, with
probability at least $1-\delta(\gamma\tau - \E\vnorm{w}_\A^*)$, we have $\vnorm{w}_\A^* \leq \gamma \tau$ and hence $\hat{x} - x^\star \in
C_\gamma$.
Using \eqref{expand}, if $\tau > \vnorm{w}_\A^*$, 
\begin{align*}
\vnorm{\hat{x}-x^\star}_2^2 & \leq (\tau + \vnorm{w}_\A^*)\vnorm{\hat{x}-x^\star}_\A \leq \frac{(1+\gamma) \tau}{\gamma\phi_\gamma(x^\star,\A)}\vnorm{\hat{x}-x^\star}_2
\end{align*}
So, with probability at least $1-\delta(\gamma\tau - \E\vnorm{w}_\A^*)$:
\begin{equation*}\belowdisplayskip=-10pt
\vnorm{\hat{x}-x^\star}_2^2  \leq \frac{(1+\gamma)^2 \tau^2}{\gamma^2 \phi_\gamma(x^\star,\A)^2}
\end{equation*}
\end{proof}

The main difference between~\eq{fast_rate_phi} and~\eq{slow-mse} is that the
MSE is controlled by $\tau^2$ rather than $\tau \| x^*\|_{\A}$.
As we will now see~\eq{fast_rate_phi} provides minimax optimal rates for the
examples of sparse vectors and low-rank matrices.

\emph{Example: Sparse Vectors in Noise}
Let $\A$ be the set of signed canonical basis
vectors in $\R^n$. In this case, $\conv(\A)$ is the unit cross polytope and the
atomic norm $\vnorm{\cdot}_\A$, coincides with the $\ell_1$ norm, and the dual
atomic norm is the $\ell_\infty$ norm. Suppose $x^\star \in \R^n$ and $T :=
\supp(x^\star)$ has cardinality $k$. Consider the problem of estimating
$x^\star$ from $y = x^\star + w$ where $w \sim \mathcal{N}(0,\sigma^2 I_n).$


\begin{prop}
Let $\A = \{\pm e_1, \ldots, \pm e_n\},$  be the set of signed canonical unit vectors in $\R^n$. Suppose $x^\star \in \R^n$ has $k$ nonzeros.  Then $\phi_\gamma(x^\star,\A) \geq \frac{(1-\gamma)}{2\sqrt{k}}$.
\end{prop}

\begin{proof}
Let $z \in C_\gamma(x^\star,\A).$ For some $\alpha>0$ we have,
\[
\vnorm{x^\star + \alpha z}_1 \leq \vnorm{x^\star}_1 + \gamma \vnorm{\alpha z}_1
\]
In the above inequality, set $z = z_T + z_{T^c}$ where $z_T$ are the components on the support of $T$ and $z_{T^c} $ are the components on the complement of $T$. Since $x^\star + z_T$ and $z_{T^c}$ have disjoint supports, we have,
\begin{align*}
\vnorm{x^\star + \alpha z_T}_1 + \alpha \vnorm{z_{T^c}}_1 \leq \vnorm{x^\star}_1 + \gamma \vnorm{\alpha z_T}_1 + \gamma \vnorm{\alpha z_{T^c}}_1\,.
\end{align*}
This inequality  implies
\begin{align*}
 \vnorm{z_{T^c}}_1 \leq \frac{1+\gamma}{1-\gamma} \vnorm{z_T}_1\,
\end{align*}
that is, $z$ satisfies the null space property with a constant of $\tfrac{1+\gamma}{1-\gamma}.$ Thus,
\[
\vnorm{z}_1 \leq \frac{2}{1-\gamma}\vnorm{z_T}_1 \leq \frac{2\sqrt{k}}{1-\gamma}\vnorm{z}_2
\]
This gives the desired lower bound. We have therefore shown that in this case $\phi_\gamma(x^\star,\A)
>\frac{(1-\gamma)}{2\sqrt{k}}$. We also have $\tau_0 = \E \vnorm{w}_\infty \geq
\sigma\sqrt{2\log(n)}.$ Pick $\tau > \gamma^{-1} \tau_0$ for some $\gamma < 1.$
Then, using our lower bound for $\phi_\gamma$ in \eq{fast_rate_phi}, we get a
rate of
\begin{align}\label{eq:sparse-fast-rate}
\frac{1}{n}\vnorm{\hat{x}-x^\star}_2^2 = O\left(\frac{\sigma^2 k\log(n)}{n}\right)
\end{align}
for the AST estimate with high probability. This bound coincides with
the minimax optimal rate derived by Donoho and Johnstone~\cite{Donoho94}. Note
that if we had used~\eq{slow-mse} instead, our MSE would have
instead been $O\left(\sqrt{\sigma^2 k\log n}\|x^\star\|_2/n\,
\right)$, which depends on the norm of the input signal $x^\star$.
\end{proof}

\emph{Example: Low Rank Matrix in Noise}
Let $\A$ be the manifold of unit norm rank-$1$ matrices in $\C^{n\times n}$. In
this case, the atomic norm $\vnorm{\cdot}_\A$, coincides with the nuclear norm
$\vnorm{\cdot}_*$, and the corresponding dual atomic norm is the spectral norm
of the matrix. Suppose $X^\star \in \C^{n\times n}$ has rank $r$, so it can be
constructed as a combination of $r$ atoms, and we are interested in estimating
$X^\star$ from $Y = X^\star + W$ where $W$ has independent
$\mathcal{N}(0,\sigma^2)$ entries.

\begin{prop}
Let $\A$ be the manifold of unit norm rank-$1$ matrices in $\C^{n\times n}$. Suppose $X^\star \in \C^{n\times n}$ has rank $r$.  Then $\phi_\gamma(X^\star,\A) \geq \frac{1-\gamma}{2\sqrt{2r}}$. 
\end{prop}

\begin{proof}
Let $U \Sigma V^H$ be a singular value decomposition of $X^\star$ with $U \in \C^{n \times r}$, $V \in \C^{n \times r}$ and $\Sigma \in \C^{r \times r}$. Define the subspaces 
\begin{align*}
T &= \{U X + Y V^H ~:~ X, Y \in \C^{n \times r} \}\\
T_0 &= \{U M V^H ~:~ M \in \C^{r \times r} \}
\end{align*}
and let $\mathcal{P}_{T_0}$, $\mathcal{P}_{T}$, and $\mathcal{P}_{T^\perp}$ be projection operators that respectively map onto the subspaces $T_0$, $T$, and the orthogonal complement of $T$. Now, if $Z \in C_\gamma(X^\star, \A)$, then for some $\alpha > 0$,  we have
\begin{align}
\label{eq:cone_matrix}
\vnorm{X^\star + \alpha Z}_* \leq 
\vnorm{X^\star}_* + \gamma \alpha \vnorm{Z}_*
\leq \vnorm{X^\star}_* + \gamma \alpha \vnorm{\mathcal{P}_T (Z)}_* +  \gamma \alpha \vnorm{\mathcal{P}_{T^\perp} (Z)}_*.
\end{align}
Now note that we have
\[
\vnorm{X^\star + \alpha Z}_* \geq \vnorm{X^\star + \alpha\mathcal{P}_{T_0} (Z)}_* + \alpha\vnorm{\mathcal{P}_{T^\perp} (Z)}_*
\]
Substituting this in \eq{cone_matrix}, we have,
\begin{align*}
\vnorm{X^\star + \alpha\mathcal{P}_{T_0} (Z)}_* + \alpha\vnorm{\mathcal{P}_{T^\perp} (Z)}_* 
\leq \vnorm{X^\star}_* + \gamma \alpha \vnorm{\mathcal{P}_T (Z)}_* +  \gamma \alpha \vnorm{\mathcal{P}_{T^\perp} (Z)}_*.
\end{align*}
Since $\vnorm{\mathcal{P}_{T_0} (Z)}_* \leq \vnorm{\mathcal{P}_T (Z)}_*$, we have 
\[
\vnorm{\mathcal{P}_{T^\perp} (Z)}_* \leq \frac{1+\gamma}{1-\gamma}\vnorm{\mathcal{P}_T(Z)}_*.
\]
Putting these computations together gives the estimate
\begin{align*}
\vnorm{Z}_* &\leq \vnorm{\mathcal{P}_T(Z)}_* + \vnorm{\mathcal{P}_{T^\perp}(Z)}_* \leq \frac{2}{1-\gamma}\vnorm{\mathcal{P}_T(Z)}
 \leq \frac{2\sqrt{2r}}{1-\gamma}\vnorm{\mathcal{P}_T(Z)}_F
 \leq \frac{2\sqrt{2r}}{1-\gamma}\vnorm{Z}_F\,.
\end{align*}
That is, we have $\phi_\gamma(X^\star,\A) \geq \frac{1-\gamma}{2\sqrt{2r}}$ as desired.
\end{proof} 


Using this proposition, $\phi_\gamma(X^\star,\A) \geq
\frac{1-\gamma}{2\sqrt{2r}}$. To obtain an estimate for $\tau$, we note that
the spectral norm of the noise matrix satisfies $\|W\|\leq
2\sqrt{n}$ with high probability~\cite{Davidson01}. Substituting these
estimates for $\tau$ and $\phi_\gamma$ in \eq{fast_rate_phi}, we get
the minimax optimal MSE
\begin{align*}
\frac{1}{n^2}\vnorm{X-\hat{X}}_F^2 = O\left( \frac{\sigma^2 r}{n} \right).
\end{align*}

% section convergence-rate (end)

\section{Conclusion} % (fold)
\label{sec:ast:conclusion}
In this chapter, we defined atomic norms and the use of atomic norm penalty to
denoise using AST~\eqref{AST} which may be thought of as a infinite dimensional
version of Lasso. We discussed how the regularization parameter may be chosen
for AST. We established a universal convergence rate which holds for all atomic
sets and all signals and saw general conditions under which a fast rate is
possible. In the following chapters, we will use this framework and apply to the
problems of line spectral estimation and system identification.
% section ast:conclusion (end)
