%!TEX root = ../dissertation.tex
\chapter{Introduction}

We live in a world of data abundance. Due to advancements in data collection,
and massive storage capabilities, we now have a dizzying amount of high
dimensional data to analyze. The task of the data scientist is to infer a simple
model to describe the data. Fortunately, many naturally occurring phenomena
often have a simple structure. This allows us to efficiently represent them
using a few building blocks or features, even if they are observed in a high
ambient dimension. The challenge is to exploit the simplicity of these objects
and recover them robustly from limited measurements. This can be a formidable
task even for simple instances of the problem.

For example, imagine a data series composed of $n$ measurements that can be
described by a linear model, in terms of a large number $p \gg n$ of potential
predictor variables or features. Finding the linear model from these limited
measurements is a hopelessly underdetermined problem. However, when the linear
model is \emph{simple} and it is known a priori that only a small subset of the
features are actually active, the problem of determining the linear model is no
longer ill posed. A simple description of the data series expresses the data
series in terms of a few active features and corresponding feature weights. Our
task of choosing the ``right'' model for the data can be cast as a combinatorial
\emph{feature selection} problem with the objective of finding the smallest
subset of features that describe the data well. This is called the ``small n,
large p'' problem in statistics and such datasets are ubiquitous - For example,
a biologist might want to identify the active genes by looking at only a few
samples of a microarray experiment which simultaneously measures thousands of
genes. Machine learning practitioners might want to infer the active features
from a large feature space with a relatively small number of measurements.

A \naive{} algorithm would explore all the possible subsets of features in
increasing order of size till we find a solution. There does not appear to be an
efficient algorithm for this as there are an exponential number of subsets to
choose from. In fact, this selection problem is provably
\textsc{NP-Hard}\footnote{Informally, this means that it is at least as hard as
a large number of well known problems in complexity theory, widely believed to
have no algorithm that can solve them in a time proportional to any polynomial
function of the number of observations.}~\cite{Natarajan95}. This does not
however preclude the possibility of designing efficient algorithms that work on
\emph{most} instances. In fact, there is now a large body of literature on the
theoretical understanding of the surprising success of convex relaxation methods
in efficiently solving the sparse recovery problem most of the time.

Instead of the hard combinatorial problem of directly minimizing sparsity of the
feature weights, the idea of using the $\ell_1$ norm of the feature weights as a
convex proxy was known to several early practitioners in Geophysics and
seismology~\cite{claerbout:robust,taylor:deconvolution,levy:spike-train,santosa}.
The authors were also aware of the robustness of $\ell_1$ regularization to
outliers and noise. This was introduced in statistics as a sparsity inducing
regularization in~\cite{tibshirani96} and in Signal Processing as a means of
exact and robust decomposition by~\cite{chen98} and subsequently studied by
Donoho and his coworkers~\cite{donoho:huo,spark}. Since the seminal publications
of ~\cite{CRT06,meinshausen:variable-selection}, which rigorously established
the exact recovery and model selection properties, the study of convex penalties
for feature selection has been a subject of intense research. This theory was
extended to other high dimensional structures including group sparse
vectors\cite{group:lasso} and low rank matrices~\cite{Recht10}.

% and cut matrices~\cite{cut:goemans}. TODO: More examples. Excellent Point Gongguo!

%This can be thought of as a model selection or a feature selection problem.
% Problems involving estimation of high dimensional parameters under corrupted and limited linear measurements are ubiquitous.

The unifying theme in different structures in high dimension is some notion of
simplicity like sparsity for vectors or rank for matrices. This thesis builds on
a recent publication~\cite{crpw} which discusses the notion of simple objects as
a sparse combination of atoms from a possibly infinite dictionary. This unifies
several related problems in sparse recovery and approximation. Simple objects
may be expressed as a sparse linear combination of a few basic atoms drawn from
a possibly infinite dictionary of atoms. For instance, a sparse vector in high
dimension is a combination of a few canonical unit vectors. A low rank matrix is
a combination of a few rank-1 matrices. A signal with a finite discrete spectrum
is a combination of a few frequencies. The authors propose using ``atomic norm''
as a convex heuristic to recover simple models from linear measurements. The
atomic norm may be thought of as a convex proxy to the combinatorial objective
function that arises naturally in sparse recovery problems.

Furthermore, the atomic norm framework allows us to naturally extend convex
relaxations to work on infinite dimensional objects which cannot be handled
satisfactorily using standard finite dimensional Lasso. We will see how to
denoise with atomic norms and then apply these ideas to revisit two fundamental
signal processing problems -- line spectral estimation and system
identification, from the perspective of convex methods. I will also discuss
efficient algorithms for these problems.

Line Spectral Estimation involves estimating frequencies and amplitudes from a
limited number of noisy measurements. This is a fundamental problem in signal
processing and there are a number of classical algorithms dating back to Prony.
Surprisingly, using convex methods in this thesis, we can empirically outperform
these classical techniques and also theoretically show near minimax optimal
performance.

Another foundational problem in Signals and Systems theory is that of inferring
a linear system from observed measurements. I will show that we can robustly
recover a low order system from a limited set of noisy measurements, using the
convex methods discussed in the thesis. We will see that it compares favorably
in terms of prediction errors to the popular subspace ID method.

\section*{Contributions and Organization} % (fold)
\label{sec:contributions}

In \textbf{Chapter~\ref{chap:ast}}, I will present the atomic norm framework in
depth and show how it unifies many linear inverse problems in high dimensional
statistics. Extending this framework, I will introduce a general regularized
estimator using the atomic norm penalty which we will call Atomic Norm Soft
Thresholding (AST). This may be thought of as an infinite dimensional version of
the Lasso~\cite{tibshirani96}. I will first establish universal properties of
the estimator and indicate when accelerated convergence rates are possible. The
choice of the regularization parameter for AST depends upon extremal properties
of the dual atomic norm of noise, and we will examine general techniques for
estimating the regularization parameter.

In \textbf{Chapter~\ref{chap:linespect}}, I show how to apply atomic norm soft
thresholding estimator to denoise line spectral signals. I will show that this
produces a consistent estimate for all signals which holds universally with very
little assumptions on the measurement model. By exploiting properties of a dual
certificate constructed in~\cite{cg_exact12,cg_noisy}, we will see that we can
achieve accelerated convergence rates when the frequencies in the line spectral
signal are well separated. As is the case for coherent designs using Lasso, this
is nearly minimax optimal. This result may be thought of as a local version of
coherence -- Although our dictionary is highly coherent, as long as the signal
we wish to recover is composed of relatively incoherent frequencies, it is
possible to recover it robustly.

I will also show that the frequencies localized by AST tend to be near the true
frequencies. With extensive experiments, I will demonstrate that the proposal in
this thesis outperforms several classical line spectral estimation algorithms.

In \textbf{Chapter~\ref{chap:sysid}}, I will show how these techniques can be
adapted for the System Identification problem. I will describe a general
regularization problem for different kinds of linear measurements of the system.
For the special case of frequency samples, I will show how to derive finite
sample guarantees on the $\mathcal{H}^2$ prediction error of the transfer
function of the linear system using AST.

Finally, in \textbf{Chapter~\ref{chap:algos}}, I will discuss algorithms for
AST. It turns out that we can efficiently solve AST as long as we have a
reasonably efficient algorithm to test membership in the unit ball of the atomic
norm. For the case of Fourier measurements, the atomic norm balls
can be characterized by a semidefinite program (SDP), which can be readily
solved using any of the various SDP solvers. While the positive case is
classical and is well known, the general case is a non-trivial extension. I
will describe a fast parallelizable algorithm for the SDP using Alternating
Directions Method of Multipliers (ADMM), and also provide an alternative
efficient discretized version of AST which can be used in the absence of
semidefinite characterizations, and in general for large problem sizes. This
boils down to solving a Lasso problem on a grid. I will show the convergence of
the Lasso solution which provides justification for discretization and Lasso on
a grid as a general computational strategy.

% section contributions (end)