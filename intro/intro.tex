%!TEX root = ../dissertation.tex
\chapter{Introduction}

We live in a world of data abundance. Due to advancements in data collection,
and massive storage capabilities, we now have a dizzying amount of high
dimensional data to analyze. The task of the data scientist is to infer a simple
model to describe the data. Fortunately, many naturally occurring phenomena
often have a simple structure. This allows us to efficiently represent them
using a few atoms or features, even if they are observed in a high ambient
dimension. The challenge is to exploit the simplicity of these objects and
recover them robustly from limited linear measurements. This can be a formidable
task even for simple instances of the problem.

For example, imagine a data series of $n$ measurements that can be described by
a linear model, in terms of a large number $p \gg n$ of potential predictor
variables. Finding the linear model from these limited measurements is a
hopelessly underdetermined problem. However, when the linear model is
\emph{simple} and it is known a priori that only a small subset of the predictor
variables are actually active, it becomes possible to formulate this problem. A
simple description of the data series expresses it in terms of a sparse
combination of predictor variables. Our task of choosing the ``right'' model for
the data can be cast as a combinatorial optimization problem with the objective
of finding the smallest subset of predictor variables that describe the data
well. This is called the ``small n, large p'' problem in statistics and such
datasets are ubiquitous - For example, biologists want to characterize samples
in terms of a small subset of a large number of genes from limited measurements.
Machine learning practioners want to infer the active features from a large
feature space from relatively small number of measurements.

A \naive{} algorithm would explore all the possible subsets of features till we
find a solution. There does not appear to be an efficient algorithm for this as
there are an exponential number of subsets to choose from. In fact, this
selection problem is provably \textsc{NP-Hard}\footnote{Informally, this means
that it is at least as hard as a large number of well known problems in
complexity theory, widely believed to have no algorithm that can solve them in a
time proportional to any polynomial function of the number of
observations.}~\cite{Natarajan95}. This does not however preclude the
possibility of designing efficient algorithms that work on \emph{most}
instances. In fact, there is now a large body of literature on the theoretical
understanding of the surprising success of convex relaxation methods in
efficiently solving the sparse recovery problem most of the time.
\\
\todo{Summarize Coherence, RIP. Emphasize Local Recovery and Off grid. Emphasize flexibility. Say problems like System ID and Line Spectral Estimation are not even expressible in this framework. Rephrase everything in terms of feature weights and use the term signal if possible.}

These convex relaxation methods penalize the $\ell_1$ norm of the feature weights instead of sparsity of feature weights.

%This can be thought of as a model selection or a feature selection problem.
% Problems involving estimation of high dimensional parameters under corrupted and limited linear measurements are ubiquitous.

The idea of treating simple objects as a sparse combination of atoms from a
possibly infinite dictionary unifies several related problems in sparse
recovery and approximation. Simple objects may be expressed as a sparse linear
combination of a few basic atoms drawn from a possibly infinite dictionary of
atoms. For instance, a sparse vector in high dimension is a combination of a
few canonical unit vectors. A low rank matrix is a combination of a few rank-1
matrices. A signal with a finite discrete spectrum is a combination of a few
frequencies.  In \cite{crpw}, the authors propose using ``atomic norm'' as a
convex heuristic to recover simple models from linear measurements. The atomic
norm may be thought of as a convex proxy to the combinatorial objective
function that arises naturally in sparse recovery problems.


Contributions and Organization of the Thesis.