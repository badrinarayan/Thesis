%!TEX root = ../dissertation.tex
\chapter{Line Spectrum Estimation}
\label{chap:linespect}

\section{Introduction}

Extracting the frequencies and relative phases of a superposition of complex
exponentials from a small number of noisy time samples is a foundational problem
in statistical signal processing. These \emph{line spectral estimation} problems
arise in a variety of applications, including the direction of arrival
estimation in radar target identification~\cite{radar}, sensor array signal
processing~\cite{arrays} and imaging systems~\cite{imaging} and also underlies
techniques in ultra wideband channel estimation~\cite{uwb},
spectroscopy~\cite{nmr}, molecular dynamics~\cite{andrade2012}, and power
electronics~\cite{power}


Despite of hundreds of years of research on the fundamental problem of line
spectrum estimation, there still remain several open questions in this area.
This chapter addresses a central one of these problems: how well can we
determine the locations and magnitudes of spectral lines from noisy temporal
samples? We establish lower bounds on how well we can recover such signals and
demonstrate that these worst case bounds can be nearly saturated by solving a
convex programming problem. Moreover, we prove that the estimator approximately
localizes the frequencies of the true spectral lines.

While polynomial interpolation using Prony's technique can estimate the
frequency content of a signal \emph{exactly} from as few as $2k$ samples if
there are $k$ frequencies, Prony's method is inherently unstable due to
sensitivity of polynomial root finding. Several methods have been proposed to
provide more robust polynomial interpolation~\cite{music,esprit,hua02} (for an
extensive bibliography on the subject, see ~\cite{stoica93}), and these
techniques achieve excellent noise performance in moderate noise. However, the
denoising performance is often sensitive to the model order estimated, and
theoretical guarantees for these methods are all asymptotic with no finite
sample error bounds. Motivated by recent work on atomic norms~\cite{crpw}, we
propose a convex relaxation approach to denoise a mixture of complex
exponentials, with theoretical guarantees of noise robustness and a better
empirical performance than previous subspace based approaches.

Specializing the denoising results of the previous chapter to the line spectral
estimation, we provide mean-squared-error estimates for denoising line spectra
with the atomic norm. The denoising algorithm amounts to soft thresholding the
noise corrupted measurements in the atomic norm and we thus refer to the problem
as \emph{Atomic norm Soft Thresholding} (AST). Furthermore, we show that AST
achieves near minimax rates for estimating line spectral signals when the
frequencies are reasonably well separated. We also give bounds on how well we
can localize the frequencies using our technique.

\subsection{Outline and summary of results} 

\subsubsection{Denoising line spectral signals.} 

We specialize the results of the abstract denoising problem in the previous
chapter to line spectral estimation in Section~\ref{sec:denoise-trig-moments}.
Consider the continuous time signal $x^\star(t), t\in \R$ with a line spectrum composed of $k$ unknown frequencies $\omega_1^\star, \ldots, \omega_k^\star$ bandlimited to $[-W,W]$. Then the Nyquist samples of the signal are given by
\begin{equation}
\label{eq:signal}
 x^\star_m := x^\star\left(\tfrac{m}{2W}\right) = \sum_{l=1}^k c_l^\star e^{i 2 \pi m f_l^\star}, m = 0, \ldots, n-1
\end{equation}
where $c_1^\star, \ldots, c_k^\star$ are unknown \emph{complex} coefficients and
$f_l^\star = \tfrac{\omega_l^\star}{2W}$ for $l = 1, \ldots, k$ are the
normalized frequencies. By swapping the roles of frequency and time or space,
the signal model \eqref{eq:signal} also serves as a proper model for
superresolution imaging where we aim to localize temporal events or spatial
targets from noisy, low-frequency measurements~\cite{cg_exact12,cg_noisy}.So,
the vector $x^\star = [x^\star_0 ~ \cdots ~ x^\star_{n-1}]^T \in \C^n$ can be
written as a non-negative linear combination of $k$ points from the set of atoms
\[
\A = \left\{e^{i 2\pi \phi}[1 ~ e^{i2\pi f} ~ \cdots ~ e^{i2\pi (n-1) f}]^T,\, f \in [0,1], \phi \in [0,1] \right\}.
\] 
The set $\A$ can be viewed as an infinite dictionary indexed by the continuously
varying parameters $f$ and $\phi$. When the number of observations, $n$, is much
greater than $k$, $x^\star$ is $k$-sparse and thus line spectral estimation in
the presence of noise can be thought of as a sparse approximation problem. 

Our first result is a global error rate that holds for line spectral signals by
specializing the results in the previous chapter. In particular, we can apply
AST and choose the regularization parameter for the strongest guarantee in
Theorem~\ref{cor:expected-mse} in terms of the expected dual norm of the noise.
This can be explicitly computed for many noise models. For example, when the
noise is Gaussian, we have the following theorem for the MSE:

\begin{theorem}
\label{thm:expmsels}

Assume $x^\star \in \C^n$ is given by $x_m^\star = \sum_{l=1}^k{c_l^\star
e^{i2\pi m f_l^\star}}$ for some unknown complex numbers $c_1^\star, \ldots,
c_k^\star$, unknown normalized frequencies $f_1^\star, \ldots, f_k^\star \in
[0,1]$ and $w \in \mathcal{N}(0,\sigma^2 I_n)$. Then the estimate $\hat{x}$ of
$x^\star$ obtained from $y=x^\star+w$ given by the solution of atomic soft
thresholding problem \eqref{eq:linespect:ast} with $\tau = \sigma \sqrt{n \log(n)}$ has the
asymptotic MSE \belowdisplayskip=-10pt \[ \frac{1}{n} \E \vnorm{\hat{x} -
x^\star}_2^2 \lesssim \sigma\sqrt{\frac{\log(n)}{n}}\sum_{l=1}^k |c_l^\star|. \]
\end{theorem}

It is instructive to compare this to the trivial estimator $\hat{x} = y$ which
has a per-element MSE of $\sigma^2$. In contrast, Theorem~\ref{thm:expmsels}
guarantees that AST produces a consistent estimate when $k =
o\left(\sqrt{n/\log(n)}\right)$. While this rate holds for any line spectral
signal, AST can perform considerably better when the frequencies are well
separated.

\begin{theorem}
\label{main}
Suppose the line spectral signal $x^\star$ is given by \eqref{eq:signal} and we
observe $n$ noisy consecutive samples $y_j = x^\star_j + w_j$ where $w_j$ is
i.i.d. complex Gaussian with variance $\sigma^2$. If the frequencies
$\{f_l\}_{l=1}^k$ in $x^\star$ satisfy a minimum separation condition
\begin{equation}
\label{min-sep}
\min_{p\neq q}d(f_p,f_q) > 4/n
\end{equation}
with $d(\cdot, \cdot)$ the distance metric on the torus, then we can determine an estimator $\hat{x}$ satisfying
\begin{equation}
\label{fast-rate}
\frac{1}{n}\vnorm{\hat{x} - x^\star}_2^2 = O\left( \sigma^2 \frac{k \log(n)}{n}\right)  
\end{equation}
with high probability by solving a semidefinite programming problem.
\end{theorem}

Note that if we exactly knew the frequencies $f_j$, the best rate of estimation
we could achieve would be $O(\sigma^2 k / n)$~\cite{oracle_lasso}. Our upper
bound is merely a logarithmic factor larger than this rate. On the other hand,
we will demonstrate via minimax theory that a logarithmic factor is unavoidable
when the support is unknown. Hence, our estimator is nearly minimax optimal.

It is instructive to compare our stability rate to the optimal rate achievable
for estimating a sparse signal from a finite, discrete
dictionary~\cite{cd_minimax}. In the case that there are $p$ incoherent
dictionary elements, no method can estimate a $k$-sparse signal from $n$
measurements corrupted by Gaussian noise at a rate less than $O(\sigma^2
\frac{k\log(p/k)}{n})$. In our problem, there are an infinite number of
candidate dictionary elements and it is surprising that we can still achieve
such a fast rate of convergence with our highly coherent dictionary. We
emphasize that none of the standard techniques from sparse approximation can be
immediately generalized to our case. Not only is our dictionary infinite, but
also it does not satisfy the usual assumptions such as restricted eigenvalue
conditions~\cite{rest_eig} or coherence conditions~\cite{coherence} that are
used to derive stability results in sparse approximation. Nonetheless, in terms
of mean-square error performance, our results match those obtained when the
frequencies are restricted to lie on a discrete grid.

In the absence of noise, polynomial interpolation can exactly recover a line
spectral signal of $k$ \emph{arbitrary} frequencies with as few as $2 k$
equispaced measurements. In the light of our minimum frequency separation
requirement~\eqref{min-sep}, why should one favor convex techniques for line
spectral estimation? Our stability result coupled with minimax optimality
establish that no method can perform better than convex methods when the
frequencies are well-separated. And, while polynomial interpolation and subspace
methods do not impose any resolution limiting assumptions on the constituent
frequencies, these methods are empirically highly sensitive to noise. To the
best of our knowledge, there is no result similar to Theorem~\ref{main} that
provides finite sample guarantees about the noise robustness of polynomial
interpolation techniques.


\subsubsection{Localizing the frequencies using the Dual} 

The atomic formulation not only offers a way to denoise the line spectral
signal, but also provides an efficient frequency localization method. After we
obtain the signal estimate $\hat{x}$ by solving \eqref{eq:linespect:ast}, we
also obtain the solution $\hat{z}$ to the dual problem as $\hat{z} = y -
\hat{x}$. As we shall see in Corollary 1, the dual solution $\hat{z}$ both
certifies the optimality of $\hat{x}$ and reveals the composing atoms of
$\hat{x}$. For line spectral estimation, this provides an alternative to
polynomial interpolation for localizing the constituent frequencies.

Indeed, when there is no noise, Cand\'es and Fernandez-Granda showed the dual
solution recovers these frequencies exactly under mild technical
conditions~\cite{CandesGranda}. This frequency localization technique is later
extended in ~\cite{offgrid2012} to the random undersampling case to yield a
compressive sensing scheme that is robust to basis mismatch. When there is
noise, numerical simulations show that the atomic norm minimization problem
\eqref{eq:linespect:ast} gives approximate frequency localization.

We also theoretically characterize how well spectral lines can be localized from noisy observations.  The frequencies estimated by any method will never exactly coincide with the true frequencies in the signal in the presence of noise. However, we can characterize the localization performance of our convex programming approach, and summarize this performance in Theorem~\ref{support}.

Before stating the theorem, we introduce a bit of notation. Define neighborhoods $N_j$ around each frequency $f_j$ in $x^\star$ by $N_j := \{ f \in \mathbb{T} : d(f,f_j) \leq 0.16/n\}$. Also define $F = \mathbb{T}\backslash \cup_{j=1}^k N_j$ as the set of frequencies in $\mathbb{T}$ which are not near any true frequency.  The letters $N$ and $F$ denote the regions that are \emph{near} to and \emph{far} from the true supporting frequencies.  The following theorem summarizes our localization guarantees.

\begin{theorem} \label{support} Let $\hat{x}$ be the solution to the same
semidefinite programming (SDP) problem as referenced in Theorem~\ref{main} and
$n > 256$. Let $\hat{c_l}$ and $\hat{f}_l$ form the decomposition of $\hat{x}$
into coefficients and frequencies, as revealed by the SDP. Then, there exist
fixed numerical constants $C_1,C_2$ and $C_3$ such that with high probability
\begin{enumerate}[i.)] \item $\sum_{l : \hat{f}_l \in F} |\hat{c}_l| \leq C_1
\sigma\sqrt{\frac{k^2 \log(n)}{n}}$ \item $\sum_{l : \hat{f}_l \in N_j}
|\hat{c}_l| \left\{ \min_{f_j \in T} d(f_j,\hat{f}_l) \right\}^2 \leq C_2
\sigma\sqrt{\frac{k^2 \log(n)}{n}}$ \item $\left| c_j - \sum_{l : \hat{f_l} \in
N_j} \hat{c}_l \right| \leq C_3 \sigma\sqrt{\frac{k^2 \log(n)}{n}}$. \item If
for any frequency $f_j$, the corresponding amplitude $|c_j| > C_1 \sigma
\sqrt{\frac{ k^2 \log(n)}{n}}$, then with high probability there exists a
corresponding frequency $\hat{f}_j$ in the recovered signal such that, \[ \left|
f_j - \hat{f}_j \right| \leq \frac{\sqrt{C_2/C_1}}{n}\left(\frac{|c_j|}{C_1
\sigma \sqrt{\frac{ k^2 \log(n)}{n}}} - 1\right)^{-\tfrac{1}{2}} \]
\end{enumerate} \end{theorem}

Part (i) of Theorem~\ref{support} shows that the estimated amplitudes
corresponding to frequencies far from the support are small. In practice, we
note that we rarely find any spurious frequencies in the far region, suggesting
that our bound (i) is conservative. Parts (ii) and (iii) of the theorem show
that in a neighborhood of each true frequency, the recovered signal has
amplitude close to the true signal. Part (iv) shows that the larger a particular
coefficient is, the better our method is able to estimate the corresponding
frequency. In particular, note that if $|c_j| > 2 C_1 \sigma \sqrt{\frac{ k^2
\log(n)}{n}}$, then $\left| f_j - \hat{f}_j \right| \leq \frac{\sqrt{C_2/C_1}
}{n}$. In all four parts, note that the localization error goes to zero as the
number of samples grows.

\subsubsection{Organization of this chapter}

We proceed as follows. In Section~\ref{sec:denoise-trig-moments}, we describe
how we can approach line spectral estimation using the framework of atomic
norms. Then, in Section~\ref{sec:frequency-localize}, we describe how we can
localize the frequencies using the dual problem. We discuss the choice of the
regularization parameter in~\ref{subsec:parameter} and to this end, we derive
nonasymptotic upper and lower bounds for the Gaussian width of the atomic set
for line spectral estimation. We specialize the results of the previous chapter
to derive the mean squared error rate that holds for all signals in
Section~\ref{sec:universal-mse}. We present minimax lower bounds show the best
rate that can be achieved for well separated signals in~\ref{sec:minimax}. We
then provide the proofs of Theorem~\ref{main} showing near minimax MSE and
Theorem~\ref{support} showing frequency localization guarantees in
Section~\ref{sec:proofs}. Then, in Section~\ref{sec:prony-method} we
contextualize our result in the canon of line spectral estimation and emphasize
the advantages and shortcomings of prior art. 
%, and describe the methods
% on which our analysis is built upon. We then in Section~\ref{sec:atomic-norms}
% describe the semidefinite programming approach to line spectral estimation,
% originally introduced in~\cite{btr12}, and explain how it relates to other
% recent spectrum estimation algorithms. We present minimax lower-bounds for line
% spectral estimation in Section~\ref{sec:minimax}. We then provide the proofs of
% our main results in Section~\ref{sec:proofs}. Finally, in
% Section~\ref{sec:experiments}, we empirically demonstrate that the semidefinite
% programming approach outperforms MUSIC~\cite{music} and Cadzow's
% technique~\cite{cadzow05} in terms of the localization metrics defined by parts
% (i), (ii) and (iii) of Theorem~\ref{support}.

\section{Denoising Line Spectral Signals}
\label{sec:denoise-trig-moments} 

We describe more precisely our signal model in this section. Suppose we wish to
estimate the amplitudes and frequencies of a signal $x(t), t \in \R$ given as a
mixture of $k$ complex sinusoids:
\begin{equation*}
  x ( t) =  \sum_{l = 1}^k c_l \exp ( i 2 \pi f_l t)
\end{equation*}
where $\{ c_l \}_{l = 1}^k$ are unknown complex amplitudes corresponding to
the $k$ unknown frequencies $\{ f_l \}_{l = 1}^k$ assumed to be in the torus
$\mathbb{T} = [0, 1]$. Such a signal may be thought of as a normalized band
limited signal and has a Fourier transform given by a line spectrum:
\begin{equation}
\label{mu}
\mu(f) = \sum_{l=1}^k c_l\delta(f - f_l)
\end{equation}
Denote by $x^\star$ the $n = 2m+1$ dimensional vector composed of equispaced 
Nyquist samples $\{x(j)\}_{j=-m}^m$   for $j=-m,\ldots,m$.

The goal of line spectral estimation is to estimate the frequencies and 
amplitudes of the signal $x(t)$ from the finite, noisy samples $y \in \C^n$ 
given by
\[
  y_j  =  x_j^\star + w_j\\
\]
for $-m \leq j \leq m$, where $w_j \sim \mathcal{C}\mathcal{N}(0,\sigma^2)$ is 
i.i.d. circularly symmetric complex Gaussian noise. 

We can model the line spectral observations $x^\star = [x_{-m}^\star,
\ldots,x_{m}^\star]^T \in \C^n$ as a sparse combination of atoms $a(f)$ 
which correspond to observations due to single frequencies. The atomic set in
this case consists of samples of individual sinusoids, $a_{f,\phi} \in \C^n$,
given by
\begin{equation}
\label{eq:trig-atoms} a_{f,\phi} = e^{i2\pi \phi}\begin{bmatrix}1 ~ e^{i2\pi f} ~
\cdots ~ e^{i2\pi(n-1)f} \end{bmatrix}^T\,.
\end{equation}
The  infinite set $\A = \{ a_{f,\phi}: f \in
[0,1], \phi \in [0,1] \}$ forms an appropriate collection of atoms for
$x^\star$, since $x^\star$ in \eqref{eq:signal} can be written as a sparse
nonnegative combination of atoms in $\A.$ In fact, $x^\star = \sum_{l = 1}^k
c_l^\star a_{f_l^\star,0} = \sum_{l = 1}^k |c_l^\star| a_{f_l^\star,\phi_l},$
where $c_l^\star = |c_l^\star|e^{i2\pi\phi_l}.$

The corresponding dual norm takes an intuitive form:
\begin{align}
	\|v\|_{\A}^* &= \sup_{{f,\phi}} \langle v, a_{f,\phi} \rangle=\sup_{f\in [0,1]}  \sup_{\phi \in [0,1]}  e^{i 2\pi \phi} \sum_{l=0}^{n-1} v_l e^{-2\pi i l f} =\sup_{ |z|\leq 1 }  \left| \sum_{l=0}^{n-1} v_l z^l\right|.\label{eq:dual-norm-poly}
\end{align}

In other words, $\|v\|_{\A}^*$ is the maximum absolute value attained on the
unit circle by the polynomial $\zeta \mapsto \sum_{l=0}^{n-1} v_l \zeta^l$.
Thus, in what follows, we will frequently refer to the \emph{dual polynomial} as
the polynomial whose coefficients are given by the dual optimal solution of the
AST problem defined in \eqref{AST}, reproduced here for convenience:
\begin{equation}
\label{eq:linespect:ast}
\mathop{\textrm{minimize}}_x \frac{1}{2} \vnorm{x - y}_2^2 + \tau \vnorm{x}_\A.
\end{equation}

In chapter~\ref{chap:algos}, we will study algorithms for solving AST and see that we can solve it exactly using a semidefinite program and approximate it with a Lasso estimate. In this chapter, we restrict ourselves to analyzing the performance of AST.

\section{Determining the frequencies}
\label{sec:frequency-localize}
\begin{figure}[htp]
\centering
\includegraphics[width=4in]{figures/dual_poly_inset.pdf}
% \includegraphics[width=2.3in]{figures/dual_poly_inset2}
\caption{ \textbf{Frequency Localization using Dual Polynomial}: The
actual location of the frequencies in the line spectral signal $x^\star \in
\C^{64}$ is shown in red. The blue curve is the dual polynomial
obtained by solving \eqref{eq:dual-ast} with $y = x^\star + w$ where $w$ is noise of SNR 10 dB.}

\label{fig:dual_poly_localize}
\end{figure}

As shown in Corollary~\ref{cor:dual-cert-support}, the dual solution can be
used to identify the frequencies of the primal solution. For line spectra, a frequency
$f \in [0,1]$ is in the support of the solution $\hat{x}$ of \eqref{eq:linespect:ast} if and
only if
\[
	 |\langle \hat{z}, a_{f,\phi} \rangle| =\left|\ \sum_{l=0}^{n-1} \hat{z}_l e^{-i 2\pi l f} \right| = \tau
\]
That is, $f$ is in the support of $\hat{x}$ if and only if it is a point of
maximum modulus for the dual polynomial. Thus, the support may be determined by
finding frequencies $f$ where the dual polynomial attains magnitude $\tau$.

Figure~\ref{fig:dual_poly_localize} shows the dual polynomial for
$\eqref{eq:linespect:ast}$ with $n = 64$ samples and $k = 6$ randomly chosen
frequencies. In the next section, we show how to pick T=he regularization parameter $\tau$.

\section{Choosing the regularization parameter}\label{subsec:parameter}
The choice of the regularization parameter is dictated by the noise model and we
derive the optimal choice for white gaussian noise samples in our analysis. As
noted in Theorem~\ref{cor:expected-mse}, the optimal choice of the
regularization parameter depends on the dual norm of the noise. A simple lower
bound on the expected dual norm occurs when we consider the maximum value of $n$
uniformly spaced points in the unit circle. Using the result of \cite{lr76}, the
lower bound whenever $n \geq 5$ is
\[
\sigma\sqrt{n\log(n) - \tfrac{n}{2} \log(4\pi\log(n))}\,.
\]

Using standard results on the extreme value statistics of Gaussian distribution,
we can also obtain a non-asymptotic upper bound on the expected dual norm of
noise for $n > 3$:
\[\sigma\left(1  + \frac{1}{\log(n)}\right)\sqrt{\log(n) + \log(16 \pi^3/2 \log(n))}\nonumber
\]

We show these computations in detail in the following section.

\subsection{Estimation of Gaussian Width}
\label{proof:dual-norm-bounds}

This section derives non-asymptotic upper and lower bounds on the expected dual norm of gaussian noise vectors, which are asymptotically tight upto $\log\log$ factors. Recall that the dual atomic norm of $w$ is given by $\sqrt{n}\sup_{f \in [0,1]}|W_f|$ where
\begin{equation*}
\label{ranproc}
W_f = \frac{1}{\sqrt{n}}\sum_{m=0}^{n-1}{w_m e^{-i2 \pi m f}}.
\end{equation*}

Here, the noise variables $w_1, w_2, \ldots$ are circularly symmetric
independent sequence of standard complex normal variables.

If we define two independent $i.i.d$ sequences of standard normal numbers $\{
g_k\}_1^\infty$ and $\{h_k\}_1^\infty$, note that we can write
\begin{equation}
W_f = \frac{1}{\sqrt{2 n}} \sum_{k=0}^{n-1} \left[ g_k \cos(2 \pi k f) - h_k \sin(2\pi k f) \right].
\end{equation}
Note that $W_f$ is a normal random variable with zero mean and a variance of
$1/2$.

\subsection{Upper Bound}

Let us use a $1/N$-net of the torus $\mathbb{T}$ to estimate the expectation of
$\sup_{f \in \mathbb{T}} {W_f}$. Define

\begin{equation*}
\mathbb{T} = \left\{t \in \mathbb{T} ~ \middle| ~ \abs{t-k/N} \leq 1/N \right\}.
\end{equation*}

We have
\begin{align}
\nonumber\E\left[\sup_{f \in T} |W_f| \right] & \leq \E\left[\sup_{1 \leq k \leq N} |W_{k/N}| \right] + \E\left[\sup_{f \in \mathbb{T}_k} \left|W_{f} - W_{k/N}\right| \right]\\
& \leq \sqrt{\log(N)} + \frac{2 \pi n}{\sqrt{3}N} \E\left[\sup_{f \in \mathbb{T}_k} |Y_f| \right]\label{balance}
\end{align}
where 
\begin{equation}
Y_f = \frac{\sqrt{3} N  \left(W_f - W_{k/N}\right)}{4 \pi n}.
\end{equation}
We will now use Dudley's integral inequality (See, for example
\cite{ledoux2011probability}) to bound $\E\left[\sup_{f \in \mathbb{T}_k} |Y_f|
\right]$. To proceed, let us compute the pseudometric $\rho$ of the Gaussian
process $\set{W_f}_f$ induced on the index set. For indices $t$ and $s$ in
$\mathbb{T}_k$,
\begin{align*}
\rho^2(t,s) & := \E \abs{Y_t - Y_s}^2\\
& = \frac{3 N^2}{8 \pi^2 n^2} \E \abs{X_t - X_s}^2\\
& = \frac{3 N^2}{2 \pi^2 n^3} \sum_{k=0}^{n-1}{\sin^2(\pi k (t - s))}\\
& \leq N^2 (t-s)^2.
\end{align*}
Thus, the diameter of the index set $\mathbb{T}_k$ with respect to $\rho$
\begin{align*}
\diam_\rho(\mathbb{T}_k) &:= \sup_{t,s  \in \mathbb{T}_k}\rho(t,s) = 1.
\end{align*}
Consequently the number $N(\mathbb{T}_k, \rho, \epsilon)$ of $\epsilon$ balls needed to cover $T_k$ under this metric $\rho$ is $1/\epsilon$. Now, by the application Dudley's integral inequality, we have
\begin{align}
\E\left[\sup_{t \in \mathbb{T}_k} |Y_f| \right] & \leq 24 \int_0^{\diam_\rho(T_k)} \sqrt{N(\mathbb{T}_k, \rho, \epsilon)} d\epsilon\\
 & \leq 24 \int_0^1 \sqrt{\log(1/\epsilon)} d\epsilon = 12 \sqrt{\pi}. \label{dudley}
\end{align}
Thus, from \eqref{dudley} and \eqref{balance}, we have
\begin{align*}
\E\left[\sup_{f \in \mathbb{T}} |W_f| \right] & \leq  \sqrt{\log(N)} +  \frac{ 16 \pi^{3/2} n} { N}
\end{align*}

Substituting $N = 16 n \sqrt{\pi^3 \log(n)},$ we get
\[
	\E\left[\sup_{f \in \mathbb{T}} W_f \right] \leq \left( 1 + \frac{1}{\log(n)} \right)\sqrt{\log(n) + \log(16 \pi^{3/2} \log(n))}
\]
By the concentration to mean of the Gaussian process, we have
\begin{equation}
\label{eq:gaussian-concentration}
\sup_{f \in \mathbb{T}} |\vabs{w}{a(f)}| \leq 2\E\left[\sup_{f \in \mathbb{T}} |W_f| \right]
\end{equation} with high probability.
\subsection{Lower Bound}

The covariance function of $W_f$ is
\begin{align*}
\E\left[W_{f_1} W_{f_2}^*\right] &= \frac{1}{n}\sum_{m=0}^{n-1} \exp(2\pi m (f_1 - f_2)) = e^{\pi (n - 1) (f_1-f_2)} \frac{\sin(n \pi (f_1 - f_2) ) }{ n \sin(\pi (f_2 - f_2))}.
\end{align*}
Thus, the $n$ samples $\left\{W_{m/n}\right\}_{m=0}^{n-1}$ are uncorrelated and thus independent because of their joint gaussianity. This gives a simple non-asymptotic lower bound using the known result for maximum value of $n$ independent gaussian random variables~\cite{lr76} whenever $n > 5$:
\begin{align*}
\E\left[\sup_{t \in T} \left| W_t \right| \right] &\geq \E\left[\max_{m = 0, \ldots, n-1} \Re\left( W_{m/n} \right)  \right] = \sqrt{\log(n) - \tfrac{ \log\log(n) + \log(4\pi)}{2}}.
\end{align*}

Combining this result with the upper bound, we have shown that the lower bound is asymptotically tight neglecting $\log\log$ terms.

% Since the dual norm induced by  $\A_N$ approximates the dual norm induced by $\A$, (See \ref{proof:dual-norm-approximation}), it is sufficient to compute an upper bound for $\vnorm{w}_{\A_N}^*.$ Note that $|W_f|^2$ has a chi-square distribution since $W_f$ is a Gaussian process. We establish a simple lemma about the maximum of chi-square distributed random variables.
% \begin{lemma}
% \label{lem:max-chi}
% Let $x_1,\ldots,x_N$ be complex gaussians with unit variance. Then,
% \begin{equation*}
% \E\left[\max_{1\leq i\leq N} |x_i|\right] \leq \sqrt{\log(N) + 1}.
% \end{equation*}
% \begin{proof}
% Let $x_1,\ldots,x_N$ be complex Gaussians with unit variance: $\E[ |x_i|^2]=1$.  Note that $2|x_i|^2$ is a chi-squared random variable with two degrees of freedom.   Using Jensen's inequality, also observe that
% \begin{align}\label{eq:jensen-bound}
% 	\E\left[\max_{1\leq i\leq N} |x_i|\right] & \leq  
% 	\E\left[\max_{1\leq i\leq N} |x_i|^2\right]^{1/2} \leq  
% 	\tfrac{1}{\sqrt{2}}\E\left[\max_{1\leq i\leq N} 2|x_i|^2\right]^{1/2}.
% \end{align}
% 
% Now let $z_1,\ldots, z_n$ be chi-squared random variables with $2$ degrees of freedom.  Then we have
% \begin{align*}
% 	\E\left[\max_{1\leq i\leq N} z_i\right] &= \int_0^\infty P\left[ \max_{1\leq i\leq N} z_i \geq t \right] dt\\
% 	\leq & \delta + \int_\delta^\infty P\left[ \max_{1\leq i\leq N} z_i \geq t \right] dt\\
% 	\leq & \delta +  N \int_\delta^\infty P\left[  z_1 \geq t\right] dt\\
% 	= & \delta +  N \int_\delta^\infty   \exp(-t/2) dt\\
% 	= & \delta +  2N  \exp(-\delta/2)
% \end{align*}
% Setting $\delta = 2\log(N)$ gives $\E\left[\max_{1\leq i\leq N} z_i\right] \leq 2 \log{N} + 2$.  Plugging this estimate into~\eqref{eq:jensen-bound} gives $\E\left[\max_{1\leq i\leq N} |x_i|\right] \leq  \sqrt{\log{N}+1}$.
% \end{proof}
% \end{lemma}
% Using Lemma \ref{lem:max-chi}, we can compute
% \begin{align*}
% \vnorm{w}_{\A_N}^* & = \sqrt{n}\max_{m=0,\ldots,N-1} \left| W_n\left(e^{i 2 \pi m/N}\right) \right|  \leq \sigma\sqrt{n\left(\log{N}+1\right)}
% \end{align*}
% Plugging in $N = 4\pi n \log(n)$ and using \eq{maximum-modulus} and
% \eq{grid-approx} establishes a tight upper bound.

\section{Universal Mean Squared Error Guarantee} % (fold)
\label{sec:universal-mse}

We can set the regularization parameter $\tau$ greater than the upper bound on the expected dual atomic norm, i.e., we pick $\eta \in [1,\infty]$ and let
\begin{equation}
\label{eq:tau}
\tau = \sigma\eta\left(1  +  \frac{1}{\log(n)}\right)\sqrt{n \log(n) + n\log(16\pi^{3/2}\log(n))}.
\end{equation}
The application of Theorem \ref{cor:expected-mse} with the choice $\eta = 1$ 
guarantees Mean-Squared Error consistency of AST for Line spectral Estimation.
This choice of $\tau$ then yields the asymptotic result in Theorem \ref{thm:expmsels}. However, as noted in Section~\ref{sec:convergence-rate}, faster convergence rates may be possible under some conditions, whenever $\eta > 1.$ Due to concentration to mean~\eqref{eq:gaussian-concentration}, whenever $\eta > 1,$ with overwhelming probability,
\begin{equation}
\label{eq:tau2}
\sup_{f \in \mathbb{T}} \left|\vabs{w}{a(f)}\right| \leq 2\eta^{-1}\tau.
\end{equation}
A recent result by Candes and Fernandez-Granda~\cite{CandesGranda} establishes
that in the noiseless case, the frequencies localized by the dual polynomial are
exact provided the minimum separation between the frequencies is at least $4/n$
where $n$ is the number of samples in the line spectral signal. Under similar
separation condition, numerical simulations suggest that \eqref{eq:linespect:ast} achieves
approximate frequency location in the noisy case.

In fact, we can also theoretically show that signals with well separated
frequencies are well behaved and achieve faster convergence rates. Unlike
previous work on fast rates for Lasso, our condition is on the signal instead of
the measurement operator. In fact, as frequencies can be arbitrarily close, the
measurement operator which samples line spectral signals is highly coherent and
it may be impossible to achieve robust recovery if frequencies can be close to
each other.

% section universal-mse (end)
\section{What is the best rate we can expect?}\label{sec:minimax}

Using results about minimax achievable rates for linear models~
\cite{cd_minimax,rw_minimax}, we can deduce that the convergence rate stated in
\eqref{fast-rate} is near optimal. Define the set of $k$ well separated
frequencies as

\[
\mathcal{S}_k = \left\{(f_1, \dots, f_k) \in \mathbb{T}^k ~\middle|~  d(f_p, f_q) \geq 4/n, p \neq q \right\}
\]

The expected minimax denoising error $M_k$ for a line spectral signal with
frequencies from $\mathcal{S}_k$ is defined as the lowest expected denoising
error rate for any estimate $\hat{x}(y)$ for the worst case signal $x^\star$
with support $T(x^\star) \in \mathcal{S}_k$. Note that we can lower bound $M_k$
by restricting the set of candidate frequencies to smaller set. To that end,
suppose we restrict the signal $x^\star$ to have frequencies only drawn from an
equispaced grid on the torus $T_n := \{ 4 j/n \}_{j=1}^{n/4}$. Note that any set
of $k$ frequencies from $T_n$ are pairwise separated by at least $4/n$. If we
denote by $F_n$ a $n \times (n/4)$ partial DFT matrix with (unnormalized)
columns corresponding to frequencies from $T_n$, we can write $x^\star = F_n
c^\star$ for some $c^\star$ with $\vnorm{c^\star}_0 = k$. Thus,

\begin{align*}
M_k &:= \inf_{\hat{x}}
 \sup_{
	T(x^\star) \in \mathcal{S}_k}
\frac{1}{n} \mathbb{E} \vnorm{\hat{x} - x^\star}_2^2
	\\
&\geq \inf_{\hat{x}} 
 \sup_{
	\vnorm{c^\star}_0 \leq k
	} \frac{1}{n} \mathbb{E} \vnorm{\hat{x} - F_n c^\star}_2^2\\
&\geq \inf_{\hat{c}}
 \sup_{\vnorm{c^\star}_0 \leq k} \frac{1}{n} \mathbb{E} \vnorm{F_n(\hat{c} - c^\star)}_2^2\\
&\geq  \frac{n}{4} \left\{ \inf_{\hat{c}}
 \sup_{\vnorm{c^\star}_0 \leq k}\frac{4}{n} \mathbb{E} \vnorm{\hat{c} - c^\star}_2^2\right\}\,.
\end{align*}

Here, the first inequality is the restriction of $T(x^\star)$. The second
inequality follows because we project out all components of $\hat{x}$ that do
not lie in the span of $F_n$. Such projections can only reduce the Euclidean
norm. The third inequality uses the fact that the minimum singular value of
$F_n$ is $n$ since $F_n^*F_n = n I_{{n}/{4}}$. Now we may directly apply the
lower bound for estimation error for linear models derived by Cand\'es and
Davenport. Namely, Theorem 1 of~\cite{cd_minimax} states that

\begin{align*}
\inf_{\hat{c}}
 \sup_{\vnorm{c^\star}_0 \leq k} \frac{4}{n} \mathbb{E} \vnorm{\hat{c} - c^\star}_2^2&\geq {C} \sigma^2 \frac{k \log\left(\frac{n}{4k}\right)}{\vnorm{F_n}_\mathrm{F}^2}\,.
 \end{align*} With the preceding analysis and the fact that $\vnorm{F_n}_{\mathrm{F}}^2 = n^2/4$, we can thus deduce the following theorem:
 \begin{theorem}
\label{minimax}
Let $x^\star$ be a line spectral signal as described by \eqref{eq:signal} with the support $T(x^\star) = \{f_1, \dots, f_k\} \in \mathcal{S}_k$ and $y = x^\star + w$, where $w \in \C^n$ is circularly symmetric Gaussian noise with variance $\sigma^2 I_n$. Let $\hat{x}$ be any estimate of $x^\star$ using $y$. Then,
\[
M_k = \inf_{\hat{x}}
 \sup_{
	T(x^\star) \in \mathcal{S}_k}
\frac{1}{n} \mathbb{E} \vnorm{\hat{x} - x^\star}_2^2
\geq C\sigma^2 \frac{k \log\left(\frac{n}{4k}\right)}{n}
\]
for some constant $C$ that is independent of $k$, $n$, and $\sigma$.
\end{theorem}

This theorem and Theorem~\ref{main} certify that AST is nearly minimax optimal for spectral estimation of well separated frequencies. 

\section{Proofs for well separated frequencies}
\label{sec:proofs}

In this section, there are many numerical constants. Unless otherwise specified,
$C$ will denote a numerical constant whose value may change from equation to
equation. Specific constants will be highlighted by accents or subscripts.

We describe the preliminaries and notations, and restate some recent results we
used before sketching the proof of Theorems \ref{main} and \ref{support}.

\subsection{Preliminaries} % (fold)
\label{sub:linespect:measures}
The sample $x^\star_j$ may be regarded as the $j$th trigonometric moment of 
the discrete measure $\mu$ given by \eqref{mu}:
\begin{eqnarray*}
  x_j^\star & = & \int_0^1 e^{i 2 \pi j f} \mu ( d f)
\end{eqnarray*}
for $-m \leq j \leq m$.
Thus, the problem of extracting the frequencies and amplitudes from noisy 
observations may be regarded as the inverse problem of estimating a measure 
from noisy trigonometric moments.

We can write the vector $x^\star$ of observations $[x_{-m}^\star, \ldots, x_m^\star]^T$ in terms of an \emph{atomic decomposition}
\[
x^\star = \sum_{l=1}^k c_l a(f_l)
\]
or equivalently in terms of a corresponding \emph{representing measure} $\mu$ given by \eqref{mu} satisfying
\[
x^\star = \int_0^1 a(f) \mu(df)
\]
There is a one-one correspondence between atomic decompositions and representing
measures. Note that there are infinite atomic decompositions of $x^\star$ and
also infinite corresponding representing measures. However, since every
collection of $n$ atoms is linearly independent, $\A$ forms a full spark
frame~\cite{spark} and therefore the problem of finding the sparsest
decomposition of $x^\star$ is well-posed if there is a decomposition which is at
least $n/2$ sparse.

The atomic norm of a vector $z$ defined in \eqref{def-atnorm} is the minimum
total variation norm~\cite{cs_otg,tvnorm} $\vnorm{\mu}_{\mathrm{TV}}$ of all
representing measures $\mu$ of $z$. So, minimizing the total variation norm is
the same as finding a decomposition that achieves the atomic norm.

% subsection linespect:measures (end)

\subsection{Dual Certificate and Exact Recovery}

Atomic norm minimization attempts to recover the sparsest decomposition by
finding a decomposition that achieves the atomic norm, i.e., find ${c_l,f_l}$
such that $x^\star = \sum_l c_l a(f_l)$ and $ \vnorm{x^\star}_\A = \sum_l |c_l|
$ or equivalently, finding a representing measure $\mu$ of the form \eqref{mu}
that minimizes the total variation norm $ \vnorm{\mu}_{\mathrm{TV}}$. The
authors of~\cite{cg_exact12} showed that when $n > 256$, the decomposition that
achieves the atomic norm is the sparsest decomposition by explicitly
constructing a dual certificate~\cite{dualcert} of optimality, whenever the
composing frequencies $f_1, \ldots, f_k$ satisfy a minimum separation
condition~\eqref{min-sep}. In the rest of the chapter, we always make the
technical assumption that $n > 256$. The following is just a restatement of Definition~\ref{def:dual-certificate} for trigonometric moments:

\begin{definition}[Dual Certificate]
\label{dual-cert}
A vector $q \in \C^n$ is called a dual certificate for the decomposition \[
x^\star = \sum_{l=1}^k c_l a(f_l)
\] if for the corresponding trigonometric polynomial $Q(f) := \langle q, a(f) \rangle$, we have
$$Q(f_l) = \operatorname{sign}(c_l), l = 1, \ldots, k$$  and $$|Q(f)| < 1$$ whenever $f\not\in \{ f_1, \ldots, f_k\}$.
\end{definition}
The authors of ~\cite{cg_exact12} not only explicitly constructed 
such a certificate characterized by the dual polynomial $Q$, but also showed that their construction satisfies some stability conditions, which is crucial for showing that denoising using the atomic norm provides stable recovery in the presence of noise.

\begin{theorem}[Dual Polynomial Stability, Lemma 2.4 and 2.5 in \cite{cg_noisy}]
\label{dual-stab} For any $f_1, \ldots, f_k$ satisfying the separation condition \eqref{min-sep} and any sign vector $v \in \C^k$ with $|v_j|=1$, there exists a trigonometric polynomial $Q = \left<q, a(f)\right>$ for some $q \in \C^n$ with the following properties: 
\begin{enumerate}
\item For each $j = 1, \ldots, k$, $Q$ interpolates the sign vector $v$ so that $Q(f_j) = v_j$
\item In each neighborhood $N_j$ corresponding to $f_j$ defined by
$N_j = \left\{ f : d(f, f_j) < {0.16}/{n} \right\}$, 
the polynomial $Q(f)$ behaves like a quadratic and there exist constants $C_a, C_a'$ so that
\begin{align}
\label{q1}|Q(f)| & \leq 1 - \frac{C_a}{2} n^2 (f-f_j)^2\\
\label{q2}|Q(f) - v_j| & \leq \frac{C_a'}{2} n^2 (f - f_j)^2
\end{align}
\item When $f \in F = [0,1] \backslash \cup_{j=1}^k{N_j}$, there is a numerical constant $C_b>0$ such that
\[
|Q(f)| \leq 1 - C_b
\]
\end{enumerate}
\end{theorem}

We use results in~\cite{cg_noisy} and~\cite{btr12} and borrow several ideas from
the proofs in~\cite{cg_noisy}, with nontrivial modifications to establish the
error rate of atomic norm regularization.

\subsection{Near optimal MSE}

In this section, we will prove Theorem~\ref{main}. Let $\hat{\mu}$ be the
representing measure for the solution $\hat{x}$ of \eqref{AST} with
minimum total variation norm, that is,
\[
\hat{x} = \int_0^1 a(f) \hat{\mu}(df)
\]

and $\vnorm{\hat{x}}_\A = \vnorm{\hat{\mu}}_{\mathrm{TV}}$. Denote the error
vector by $e = x^\star - \hat{x}$. Then, the difference measure $\nu = \mu -
\hat{\mu}$ is a representing measure for $e$. We first express the denoising
error $\vnorm{e}_2^2$ as the integral of the error function $E(f) = \langle e,
a(f) \rangle,$ against the difference measure $\nu$:
\begin{align*}
\vnorm{e}_2^2 &= \langle e, e \rangle\\
& = \left\langle e, \int_0^1 a(f) \nu(df) \right\rangle\\
& =  \int_0^1  \left\langle e,a(f) \right\rangle \nu(df)\\
& = \int_0^1 E(f) \nu(df).
\end{align*}
Using a Taylor series approximation in each of the near regions $N_j$, we first
show that the denoising error (or in general any integral of a trigonometric
polynomial against the difference measure) can be controlled in terms of an
integral in the far region $F$ and the zeroth, first, and second moments of the
difference measure in the near regions. The precise result is presented in the
following lemma:
\begin{lemma}
\label{part1}
Define
\begin{align*} 
I_0^j &:= \left| \int_{N_j} \nu(df) \right|\\
I_1^j &:= n \left| \int_{N_j} (f-f_j) \nu(df) \right|\\
I_2^j &:= \frac{n^2}{2} \int_{N_j} (f-f_j)^2 |\nu|(df)\\
I_l &:= \sum_{j=1}^k I_l^j,~~\mbox{for}~l = 0, 1, 2\,.
\end{align*}
Then for any $m$th order trigonometric polynomial $X$, we have
\[
\int_0^1{ X(f) \nu(df)}
\leq \vnorm{X(f)}_\infty \left(\int_F{|\nu|(df)} + I_0 + I_1 + I_2\right)
\]
\end{lemma}
\begin{proof}\label{apx:pf:taylor}
We first split the domain of integration into the near and far regions.
\begin{align}
\left |\int_0^1 X(f) \nu(df)\right | 
&\leq \left |\int_F X(f) \nu (f)\right | + \sum_{j=1}^k \left | \int_{N_j}X(f) \nu(df)\right |\nonumber \\
&\leq \vnorm{X(f)}_\infty \int_F |\nu| (df) + \sum_{j=1}^k \left | \int_{N_j}X(f) \nu(df) \right |.\label{Xfbd}
\end{align}
by using H\"{o}lder's inequality for the last inequality. Using Taylor's theorem, we may expand the integrand $X(f)$ around $f_j$ as
\[
X(f) = X(f_j) + (f-f_j) X'(f_j) + \frac{1}{2} X''(\xi_j) (f-f_j)^2 
\]
for some $\xi_j \in N_j$. 
Thus,
{\small
\begin{align*}
&|X(f)-X(f_j)-X'(f_j)(f-f_j)|\\
&\leq \sup_{\xi \in N_j} \frac{1}{2}|X''(\xi)|(f -f_j)^2\\ &\leq \frac{1}{2} n^2 \vnorm{X(f)}_\infty(f - f_j)^2, 
\end{align*}
}
where for the last inequality we have used a theorem of Bernstein for trigonometric polynomials (see, for example~\cite{bernstein}):  
\begin{align*}
|X'(f_j)|  & \leq n \vnorm{X(f)}_\infty\\
|X''(f_j)| & \leq n^2 \vnorm{X(f)}_\infty.
\end{align*}
As a consequence, we have
\begin{align*}
\left | \int_{N_j} X(f) \nu(df)\right| &\leq \left| X(f_j)\right| \left| \int_{N_j} \nu (df)\right| + \left|X'(f_j)\right| \left|\int_{N_j} (f-f_j) \nu (df)  \right|\\
& + \frac{1}{2} n^2 \|X(f)\|_\infty \int_{N_j} (f-f_j)^2 |\nu| (df) \\
& \leq \|X(f)\|_\infty \left(I_0^j + I_1^j + I_2^j\right).
\end{align*}
Substituting back into \eqref{Xfbd} yields the desired result.
\end{proof}


Applying Lemma \ref{part1} to the error function, we get
\begin{equation}
\label{ebd}
\vnorm{e}_2^2 \leq \vnorm{E(f)}_\infty 
\left( \int_F{|\nu|(df)} + I_0 + I_1 + I_2\right)
\end{equation}
As a consequence of our choice of $\tau$ in \eq{tau}, we can show that $\vnorm{E(f)}_\infty \leq (1+2\eta^{-1})\tau$ with high probability. In fact, we have
\begin{align*}
\vnorm{E(f)}_\infty &= \sup_{f \in [0,1]}\left|\langle e, a(f) \rangle\right|\\
&= \sup_{f \in [0,1]} \left| \langle x^\star - \hat{x}, a(f) \rangle\right|\\
&\leq \sup_{f \in [0,1]} \left| \langle w, a(f) \rangle \right| +  \sup_{f \in [0,1]} \left| \langle y - \hat{x}, a(f) \rangle \right|\\
&\leq \sup_{f \in [0,1]} \left| \langle w, a(f) \rangle \right| +  \tau\\
\label{errbd} \numberthis &\leq (1 +2\eta^{-1})\tau \leq 3 \tau, \text{with high probability.}
\end{align*}
The second inequality follows from the optimality conditions for \eqref{AST} and
the penultimate inequality is from \eqref{eq:tau2}.

Therefore, to complete the proof, it suffices to show that the other terms on
the right hand side of \eqref{ebd} are $O(\frac{k\tau}{n})$. While there is no
exact frequency recovery in the presence of noise, we can hope to get the
frequencies approximately right. Hence, we expect that the integral in the far
region can be well controlled and the local integrals of the difference measure
in the near regions are also small due to cancellations. Next, we utilize the
properties of the dual polynomial in Theorems~\ref{dual-stab} and another
polynomial given in Theorem~\ref{dual-lin} in Appendix \ref{apx:collection} to
show that the zeroth and first moments of $\nu$ may be controlled in terms of
the other two quantities in \eqref{ebd} to upper bound the error rate. The
following lemma is similar to Lemmas 2.2 and 2.3 in~\cite{cg_noisy}, but we have
made several modifications to adapt it to our signal and noise model.
\begin{lemma}
\label{part2}
There exists numeric constants $C_0$ and $C_1$ such that
\begin{align*}
I_0 &\leq C_0 \left(\frac{k \tau}{n} + I_2 + \int_F{|\nu|(df)}\right) \\
I_1 &\leq C_1 \left(\frac{k \tau}{n} + I_2 + \int_F{|\nu|(df)}\right).
\end{align*}
\end{lemma}
\begin{proof}
\label{apx:pf:I0I1}
Consider the polar form
\[
  \int_{N_j} \nu ( df)  =  \left| \int_{N_j} \nu ( df) \right| e^{i \theta_j} .
\]
Set $v_j = e^{-i \theta_j}$ and let $Q(f)$ be the dual polynomial promised by Theorem \ref{dual-stab} for this $v$. Then, we have 
\begin{align*}
  \left| \int_{N_j} \nu ( d f) \right| & = 
  \int_{N_j} e^{- i \theta_j} \nu ( d f)\\
  & =  \int_{N_j} Q ( f) \nu ( d f) + 
  \int_{N_j} (e^{- i \theta_j} -  Q ( f) ) \nu ( d f)
\end{align*}
Summing over $j=1,\ldots,k$ yields

\begin{align}
\nonumber I_0 &= \sum_{j=1}^k \left| \int_{N_j} \nu(df) \right|\\
\nonumber & = \sum_{j=1}^k\int_{N_j} Q ( f) \nu ( d f) + 
\sum_{j=1}^k \int_{N_j} ( v_j - Q ( f)) \nu ( d f)\\
\nonumber & \leq \left|\int_0^1 Q(f) \nu(df) \right| + \int_F|\nu|(df) + C_a' I_2, \text{ using triangle inequality and \eqref{q2}}\\
\label{i0} & \leq \frac{C k \tau}{n} + \int_F|\nu|(df) + C_a' I_2, \text{ using \eqref{qv}}.
\end{align}
We use a similar argument for bounding $I_1$ but this time use the dual polynomial $Q_1(f)$ guaranteed by Theorem \ref{dual-lin}. Again, start with the polar form
\[
  \int_{N_j} (f - f_j) \nu ( d f)  =  \left|
  \int_{N_j} (f - f_j) \nu ( d f) \right| e^{i \theta_j} = I_1^j e^{i\theta_j}/n
\]
Set $v_j = e^{-i \theta_j}$ in Theorem \ref{dual-lin} to obtain
{
\begin{align*}
  I_1^j & = 
  n\int_{N_j} e^{- i \theta_j} ( f - f_j) \nu ( d f)\\
  & =  n \int_{N_j} (v_j (
  f - f_j) - Q_1 ( f)) \nu ( d f)  + n\int_{N_j} Q_1 ( f) \nu ( d f)
\end{align*}
}
Summing over $j=1,\ldots,k$ yields
{
\begin{align}
\nonumber I_1 &= \sum_{j=1}^k I_1^j\\
\nonumber &= n \sum_{j=1}^k \int_{N_j} (v_j (
  f - f_j) - Q_1 ( f)) \nu ( d f) + n\sum_{j=1}^k\int_{N_j} Q_1 ( f) \nu ( d f)\\
\nonumber &\leq C_a^1 I_2 + n\left|\int_0^1 Q_1(f) \nu(df)\right| +  n\left |\int_F Q_1(f) \nu(df)\right |\\
\label{i1}& \leq C_a^1 I_2 + \frac{C k \tau}{n} +  C_b^1 \int_F|\nu|(df)
\end{align}
}
For the first inequality, we have used \eqref{ca1} and triangle inequality, and for the last inequality, we have used \eqref{q1v} and \eqref{cb1}. Equations \eqref{i0} and \eqref{i1} complete the proof.
\end{proof}	

All that remains to complete the proof is an upper bound on $I_2$ and $\int_F{|\nu|(df)}$.  The key idea in establishing such a bound is deriving upper and lower bounds on the
difference $\| P_{T^c} ( \nu) \|_{{\mathrm{TV}}} - \| P_T ( \nu) \|_{{\mathrm{TV}}}$
between the total variation norms of $\nu$ on and off the support. The upper bound can be derived using optimality conditions. We lower bound $\| P_{T^c} ( \nu)
\|_{{\mathrm{TV}}} - \| P_{T} ( \nu) \|_{{\mathrm{TV}}}$ using the fact that a constructed dual
certificate $Q$ has unit magnitude for every element in the support
$T$ of $P_T ( \nu)$ whence we have $\| P_T ( \nu) \|_{{\mathrm{TV}}} = \int_{\mathbb{T}}
Q ( f) \nu ( d f)$. A critical element in deriving both the lower and upper bounds is that the dual polynomial $Q$ has quadratic drop in each near regions $N_j$ and is bounded away from one in the far region $F$. Finally, by combing these bounds and carefully controlling the regularization parameter, we get the desired result summarized in the following lemma. 
\begin{lemma}
Let $\tau = \eta\sigma \sqrt{n\log(n)}$. If $\eta>1$ is large enough, then there exists a numerical constant $C$ such that, with high probability
\label{part3}
\[
\int_F{|\nu|(df)} + I_2 \leq \frac{C k \tau}{n}.
\]
\end{lemma}
\begin{proof}\label{apx:pf:I2far}
Denote by $P_T(\nu)$ the projection of the difference measure $\nu$ on the support set $T = \{f_1, \ldots, f_k\}$ of $x^\star$ so that $P_T(\nu)$ is supported on $T$. Then, setting $Q(f)$ the polynomial in Theorem \ref{dual-stab} that interpolates the sign of $P_T( \nu)$, we have
{
\begin{align*}
  \| P_T ( \nu) \|_{\mathrm{TV}} & =  \int_0^1 Q ( f) P_T ( \nu) ( d f)\\
  & \leq  \left| \int_0^1 Q ( f) \nu ( d f) \right| + \left|
  \int_{T^c} Q ( f) \nu ( d f) \right|\\
  & \leq  \frac{C k \tau}{n} + \sum_{f_j \in T} \left|
  \int_{N_j / \{ f_j \}} Q ( f) \nu ( d f) \right| + \left|
  \int_F Q ( f) \nu ( d f) \right|,
\end{align*}}
where for the first inequality we used triangle inequality and for the last inequality we used \eqref{qv}. 
The integration over $F$ is can be bounded using H\"{o}lder's inequality
\[
  \left| \int_{F} Q ( f) \nu ( d f) \right|  
  \leq  ( 1 - C_b) \int_F |\nu|(df)
\]

We continue with
{
\begin{align*}
  \left| \int_{N_j / \{ f_j \}} Q ( f) \nu ( d f) \right| 
  & \leq  \left| \int_{N_j / \{ f_j \}} | Q ( f) | | \nu | ( d f) \right|\\
  & \leq  \int_{N_j / \{ f_j \}} ( 1 - \tfrac{1}{2}n^2 C_a ( f - f_j)^2) | \nu | ( d f)\\
  & \leq \int_{N_j / \{ f_j \}} | \nu | ( d f) - C_a I_2^j.
\end{align*}
}
As a consequence, we have
{
\begin{align*}
  \nonumber \| P_T ( \nu) \|_{\mathrm{TV}} & \leq  \frac{C k \tau}{n} + \sum_{f_j
  \in T} \int_{N_j / \{ f_j \}} | \nu | ( d f) - C_a
  I_2 + ( 1 - C_b) \int_F |\nu|(df) \nonumber\\
 & \leq  \frac{C k \tau}{n} + \underbrace{\sum_{f_j
  \in T} \int_{N_j / \{ f_j \}} | \nu | ( d f) + \int_F |\nu|(df)}_{\|P_{T^c}\|_{\mathrm{TV}}} - C_a
 I_2  - C_b \int_F |\nu|(df)
\end{align*}
}
or equivalently,
\begin{align}
  \label{eqn:lower}
\|P_{T^c}(\nu) \|_{\mathrm{TV}} - \|P_T(\nu)\|_{\mathrm{TV}} \geq C_a I_2 + C_b \int_F |\nu|(df) - \frac{C k\tau}{n}.
\end{align}

Now, we appeal to  the optimality conditions~\eqref{pro:optimality} of AST to obtain
\[
\vnorm{\hat{x}}_\A \leq \vnorm{x^\star}_\A - \langle w, e \rangle/\tau
\]
and thus
\begin{equation}
\label{opt-cond}
\vnorm{\hat{\mu}}_{\mathrm{TV}} \leq \vnorm{\mu}_{\mathrm{TV}} + |\langle w, e \rangle|/\tau.
\end{equation}
Using Lemma~\ref{part1},
\begin{align}
\nonumber |\langle w, e \rangle| & = |\langle w, \int_0^1 a(f) \nu(df)  |\rangle\\
& = \left|\int_0^1  \left\langle w,  a(f)  \right\rangle \nu(df)\right|\\
\nonumber & \leq \vnorm{\left\langle w,  a(f)  \right\rangle}_\infty\left(\frac{C k \tau}{n} + I_0 + I_1 + I_2\right)\\
\label{w-expand}& \leq 2\eta^{-1} \tau \left(\frac{C k \tau}{n} + I_0 + I_1 + I_2\right)\nonumber \\
 & \leq C \eta^{-1} \tau \left(\frac{k\tau}{n} + I_2 + \int_F |\nu|(df) \right)
\end{align}
with high probability, where for the penultimate inequality we used our choice
of $\tau$ and $\vnorm{\left\langle w, a(f) \right\rangle}_\infty \leq 2\eta^{-1}
\tau$ with high probability from \eqref{eq:tau2}.

Substituting \eqref{w-expand} in \eqref{opt-cond}, we get
\begin{align*}
& \vnorm{\mu}_{\mathrm{TV}} + C \eta^{-1} \tau \left(\frac{k\tau}{n} + I_2 + \int_F |\nu|(df) \right)\\
& \geq \vnorm{\hat{\mu}}_{\mathrm{TV}}\\
& = \vnorm{\mu + \nu}_{\mathrm{TV}}\\
& \geq \vnorm{\mu}_{\mathrm{TV}} - \vnorm{{P_T(\nu)}}_{\mathrm{TV}} + \vnorm{{P_{T^c}(\nu)}}_{\mathrm{TV}}\end{align*}
Canceling $\|\mu\|_{\mathrm{TV}}$ yields 
\begin{align}\label{eqn:upper}
\|P_{T^c}(\nu)\|_{\mathrm{TV}} - \|P_T(\nu)\|_{\mathrm{TV}} \leq C\eta^{-1}\tau \left(\frac{k \tau}{n} + I_2 + \int_F |\nu|(df)\right)
\end{align}
As a consequence of \eqref{eqn:lower} and \eqref{eqn:upper}, we get,
\[
  C(1+\eta^{-1}) \frac{k \tau}{n} \geq  ( C_b - \eta^{-1} C)  \int_F{|\nu|(df)} + ( C_a - \eta^{-1} C)I_2
\]
whence the result follows for large enough $\eta.$
\end{proof}

Putting together Lemmas \ref{part1}, \ref{part2} and \ref{part3}, we finally prove our main theorem:
\begin{align*}
\frac{1}{n}\vnorm{e}_2^2 
&\leq \frac{\vnorm{E(f)}_\infty}{n} \left(\int_F{|\nu|(df)} + I_0 + I_1 + I_2\right)\\
&\leq \frac{\vnorm{E(f)}_\infty}{n} \left(\frac{C_1 k \tau}{n} + C_2 \int_F{|\nu|(df)} + C_3 I_2\right)\\
&\leq  \frac{\vnorm{E(f)}_\infty}{n} \frac{C k \tau}{n} \\
& \leq \frac{C k \tau^2}{n^2}\\
&= O\left(\sigma^2\frac{k \log(n)}{n}\right).
\end{align*}

The first three inequalities come from successive applications of Lemmas 1, 2
and 3 respectively. The fourth inequality follows from \eqref{errbd} and the
fifth by our choice of $\tau$ according to Eq. \eq{tau}. This completes the
proof of Theorem \ref{main}.

\subsection{Approximate Frequency Localization}
\label{sec:support}

In this section, we will prove Theorem~\ref{support}. The first two statements
in Theorem \ref{support} are direct consequences of Lemma~\ref{part3}. For
(iii.), we follow~\cite{granda2} and use the dual polynomial $Q_j^{\star} ( f) =
\langle q_j^{\star}, a ( f)\rangle$ constructed in Lemma 2.2 of~\cite{granda2}
which satisfies

\begin{eqnarray*}
  Q_j^{\star} ( f_j) & = & 1\\
  | 1 - Q_j^{\star} ( f) | & \leq & n^2 C_1' ( f - f_j)^2, f \in N_j\\
  | Q_j^{\star} ( f) | & \leq & n^2 C_1' ( f - f_{j'})^2, f \in N_{j'}, j' \neq
  j\\
  | Q_j^{\star} ( f) | & \leq & C_2', f \in F.
\end{eqnarray*}
We note that $c_j - \sum_{\hat{f}_l \in N_j} \hat{c}_l = \int_{N_j} \nu(df)$. Then, by applying triangle inequality several times,
\begin{align*}
\left| \int_{N_j}  \nu(df)\right|
& \leq \left| \int_{N_j}  Q_j^\star (f) \nu(df)\right| + \left| \int_{N_j}  (1-Q_j^\star (f)) \nu(df)\right|\\
& \leq \left| \int_0^1  Q_j^\star (f) \nu(df)\right| + \left| \int_{N_j^c}  Q_j^\star (f) \nu(df)\right| + \left| \int_{N_j}  (1-Q_j^\star (f)) \nu(df)\right|\\
& \leq \left|\int_0^1  Q_j^\star (f) \nu(df)\right| + \left| \int_{F}  Q_j^\star (f) \nu(df)\right| \\
&\qquad\qquad\qquad + \sum_{\substack{j' \neq j\\j'=1}}^k \int_{N_{j'}} \left| Q_j^\star (f)\right| |\nu|(df) +  \int_{N_j}  \left|1-Q_j^\star (f)\right| |\nu(df)|\,.
\end{align*}

We upper bound the first term using Lemma~\ref{l4} in Appendix \ref{apx:collection} which yields
\[
\left| \int^0_{1}  Q_j^\star (f) \nu(df)\right| \leq \frac{Ck \tau}{n}
\]
The other terms can be controlled using the properties of $Q_j^\star$:
\begin{align*}
\left| \int_{F}  Q_j^\star (f) \nu(df)\right| & \leq C_2' \int_{F} |\nu| (df)\\
\sum_{\substack{j' \neq j\\j'=1}}^k \int_{N_{j'}} \left| Q_j^\star (f)\right| |\nu|(df) +  \int_{N_j}  \left|1-Q_j^\star (f)\right| |\nu|(df)
& \leq
 C_1'\sum_{j'=1}^k \int_{N_{j'}} n^2 (f-f_{j'})^2 |\nu|(df) = C_1 I_2
\end{align*}
Using Lemma~\ref{part3}, both of the above are upper bounded by $\frac{C k \tau}{n}$. Now, by combining these upper bounds, we finally have
\[
\left| c_j - \sum_{l : \hat{f}_l \in N_j} \hat{c}_l \right| \leq \frac{C_3 k \tau}{n}
\]
This shows part (iii) of the theorem. Part (iv) can be obtained by combining parts (ii) and (iii).


\section{Related Work}
\label{sec:prony-method}

% \todo{Reduce Prony}
% \todo{Prony, Music, Cadzow, Matrix Pencil only 1 para}
% \todo{Discretization Malioutov and Basis Mismatch}
% \todo{Candes}

% This
% technique is attributed to Prony in the eighteenth century.

The classical methods of line spectral estimation, often called linear
prediction methods, are built upon the seminal interpolation method of
Prony~\cite{prony1795}. In the noiseless case, with as little as $n=2k$
measurements, Prony's technique can identify the frequencies exactly, no matter
how close the frequencies are. However, Prony's technique is known to be
sensitive to noise due to instability of polynomial rooting~\cite{kahn92}.
Following Prony, several methods have been employed to robustify polynomial
rooting method including the Matrix Pencil algorithm~\cite{hua02}, which
recasts the polynomial rooting as a generalized eigenvalue problem and cleverly
uses extra observations to guard against noise. The MUSIC~\cite{music} and
ESPRIT~\cite{esprit} algorithms exploit the low rank structure of the
autocorrelation matrix.


Cadzow~\cite{cadzow02} proposed a heuristic that improves over MUSIC by 
exploiting the Toeplitz structure of the matric of moments by alternately
projecting between the linear space of Toeplitz matrices and the space of rank
$k$ matrices where $k$ is the desired model order.
Cadzow's technique is very similar~\cite{ssa_special_issue} to a popular
technique in time series literature~\cite{tsbook1,tsbook2} called Singular
Spectrum Analysis~\cite{ssa}, which uses autocorrelation matrix instead
of the matrix of moments for projection. Both these techniques may be viewed as
instances of structured low rank approximation~\cite{chu2003structured} which
exploit additional structure beyond low rank structure used in subspace based
methods such as MUSIC and ESPRIT. Cadzow's method has been identified as a  fruitful preprocessing step for linear prediction methods~\cite{blu08}. A  survey of classical linear prediction methods can be found in~\cite{blu08,StoicaMoses} and an extensive list of references is given in~\cite{stoica93}.

Most, if not all of the linear prediction methods need to estimate the model
order by employing some heuristic and the performance of the algorithm is
sensitive to the model order. In contrast, our algorithms AST and the Lasso
based method, only need a rough estimate of the noise variance. In our experiments, we provide
the true model order to Matrix Pencil, MUSIC and Cadzow methods, while we use
the estimate of noise variance for AST and Lasso methods, and still compare favorably to the classical line spectral methods.

In contrast to linear prediction methods, a number of authors
~\cite{chen98spectrum,malioutov05,bourguignon2007irregular}
have suggested using compressive sensing and viewing the frequency estimation
as a sparse approximation problem. For instance,~\cite{malioutov05} notes that
the Lasso based method has better empirical localization performance than the
popular MUSIC algorithm. However, the theoretical analysis of this phenomenon
is complicated because of the need to replace the continuous frequency space by
an oversampled frequency grid. Compressive sensing based results (see, for
instance,~\cite{duartescs}) need to carefully control the incoherence of their
linear maps to apply off-the-shelf tools from compressed sensing. It is
important to note that the performance of our algorithm improves as the grid
size increases. But this seems to contradict conventional wisdom in compressed
sensing because our design matrix $\Phi$ becomes more and more coherent, and
limits how fine we can grid for the theoretical guarantees to hold.

We circumvent the problems in the conventional compresssive sensing analysis by
directly working in the continuous parameter space and hence step away from
such notions as coherence, focussing on the geometry of the atomic set as the
critical feature. By showing that the continuous approach is the limiting
case of  the Lasso based methods using the convergence of the corresponding
atomic norms, we justify denoising line spectral signals using Lasso on a
large grid. Since the original submission of this manuscript,
Cand\`es and Fernandez-Granda~\cite{CandesGranda} showed that our SDP
formulation exactly recovers the correct frequencies in the noiseless case.

% To date, line spectral analysis may be broadly classified into two camps.
% \emph{Subspace methods}~\cite{music,esprit,cadzow05,ssa} build upon polynomial
% interpolation~\cite{prony1795} and exploit certain low rank structure in the
% spectrum estimation problem for denoising. Research on subspace approaches has
% yielded several standard algorithms that are widely deployed and shown to
% achieve Cram\'{e}r-Rao bound asymptotically~\cite{fri,cramer-subspace}.
% However, the sensitivity to noise and model order is not well understood, and
% there are few guarantees of how these algorithms perform given a limited number
% of noisy measurements. For a review of many of these classical approaches, see
% for example~\cite{StoicaMoses}.

More recently, approaches based on convex optimization have gained favor and
have been demonstrated to perform well on a variety of spectrum estimation
tasks~\cite{malioutov05,bourguignon2007irregular,baraniuk2010model,zweig2003irregular}. These convex programming methods restrict the frequencies to lie on a
finite grid of points and view line spectral signals as a sparse combination of
single frequencies. While these methods are reported to have significantly
better localization properties than subspace methods (see for
example,~\cite{malioutov05}) and admit fast and robust algorithms, they have
two significant drawbacks. First, while finer gridding may lead to better
performance, very fine grids are often numerically unstable. Furthermore,
traditional compressed sensing theory does not adequately characterize the
performance of fine gridding in these algorithms as the dictionary becomes
highly coherent.

Some very recent work~\cite{btr12,cg_exact12,cg_noisy} bridges the gap between
the performant discretized algorithms and continuous subspace approaches by
developing a new theory of convex relaxations for infinite continuous
dictionary of frequencies. Our work in~\cite{btr12} applies the atomic norm
framework proposed by Chandrasekaran et al~\cite{crpw} to the line spectral
estimation problem. There, we established stability results on the denoising
error and demonstrated empirically that our algorithm compared favorably with
both the classical and recent convex approaches which assume the frequencies
are on an oversampled DFT grid. Our prior results made no assumption about the
separation between frequencies. When the frequencies are well separated, the
current work demonstrates that much faster convergence rates are achieved.

Our work is closely related to recent results established by Cand\`es and
Fernandez-Granda~\cite{cg_exact12} on exact recovery using convex methods and
their recent work~\cite{cg_noisy} on exploiting the robustness of their dual
polynomial construction to show super-resolution properties of convex methods.
The total variation norm formulation used in~\cite{cg_noisy} is equivalent to
the atomic norm specialized to the line spectral estimation problem.

Robustness bounds were established in both our earlier work~\cite{btr12} and in
the work of Cand\`es and Fernandez-Granda~\cite{cg_noisy}. In~\cite{btr12}, a
slow convergence rate was established with no assumptions about the separation
of frequencies in the true signal. In~\cite{cg_noisy}, the authors provide
guarantees on the $L_1$ energy of error in the frequency domain in the case
that the frequencies are well separated. The noise is assumed to be adversarial
with a small $L_1$ spectral energy. In contrast, we show near minimax
denoising error under Gaussian noise. It is also not clear that there is a
computable formulation for the optimization problem analyzed
in~\cite{cg_noisy}. While the guarantees the authors derive in~\cite{cg_noisy}
are not comparable with our results, several of their mathematical
constructions are used in our proofs here.

Additional recent work derives conditions for approximate support recovery
under the Gaussian noise model using the Beurling-Lasso ~\cite{azais}. There,
the authors show that there is a true frequency in the neighborhood of every
estimated frequency with large enough amplitude. We note that the
Beurling-Lasso is equivalent to the atomic norm algorithm that we analyze in
this chapter. A more recent paper by Fernandez-Granda{\cite{granda2}} improves
this result by giving conditions on recoverability in terms of the true signal
instead of the estimated signal and prove a theorem similar to
Theorem~\ref{support}, but use a worst case $L_2$ bound on the noise samples.
Here, we improve these recent results in our proof of Theorem~\ref{support},
providing tighter guarantees under the Gaussian noise model.

\section{Conclusion and Future Work}\label{sec:conclusions}

The Atomic norm formulation of line spectral estimation provides several
advantages over prior approaches. By performing the analysis in the continuous
domain we were able to derive simple closed form rates using fairly
straightforward techniques. We only grid the unit circle at the very end of our
analysis and determine the loss incurred from discretization. This approach
allowed us to circumvent some of the more complicated theoretical arguments that
arise when using concepts from compressed sensing or random matrix theory.

In this paper, we demonstrated stability of atomic norm regularization by
analysis of specific properties of the atomic set of moments and the associated
dual space of trigonometric polynomials. The key to our analysis is the
existence and properties of various trigonometric polynomials associated with
signals with well separated frequencies. This work provides several interesting
possible future directions, both in line spectral estimation and in signal
processing in general. We conclude with a short outline of some of the
possibilities.

Though we have made significant progress at understanding the theoretical
limits of line-spectral estimation and superresolution, our bounds could still
be improved. For instance, it remains open as to whether the logarithmic term
in Theorem~\ref{main} can be improved to $\log(n/k)$. Deriving such an upper
bound or improving our minimax lower bound would provide an interesting
direction for future work.

Additionally, it is not clear if our localization bounds in
Theorem~\ref{support} have the optimal dependence on the number of sinusoids
$k$. For instance, we expect that the condition on signal amplitudes for
approximate support recovery should not depend on $k$, by comparison with
similar guarantees that have been established for Lasso~\cite{coherence}. We
additionally conjecture that for a large enough regularization parameter, there
will be no spurious recovered frequencies in the solution. That is, there
should be no non-zero coefficients in the ``far region'' $F$ in
Theorem~\ref{support}. Future work should investigate whether better guarantees
on frequency localization are possible.

% Our work also naturally extends to moment problems where the atomic measures are
% supported on the unit disk in the complex plane. These problems arise naturally
% in controls and systems theory and include model order reduction, system
% identification, and control design. Applying the standard program developed in
% Section~\ref{sec:abstract-denoising} provides a new look at these classic
% operator theory problems in control theory. It would be of significant
% importance to develop specialized atomic-norm denoising algorithms for control
% theoretic problems. Such an approach could yield novel statistical bounds for
% estimation of rational functions and $\mathcal{H}_\infty$-norm approximations.
	
